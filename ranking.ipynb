{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>review_count</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rule5_threshold</th>\n",
       "      <th>rule2_threshold</th>\n",
       "      <th>rule4_threshold</th>\n",
       "      <th>rule3_threshold</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>rule6_threshold</th>\n",
       "      <th>count_ones</th>\n",
       "      <th>target</th>\n",
       "      <th>funny_threshold</th>\n",
       "      <th>star_threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>661a5d4bee4d349db0518760</td>\n",
       "      <td>qVc8ODYU5SZjKXVBgXdI7w</td>\n",
       "      <td>585</td>\n",
       "      <td>Egy2a4qZeXGr2aY6KMxxbg</td>\n",
       "      <td>Remarkable food with beach access for the whol...</td>\n",
       "      <td>2017-05-09 23:21:36</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405455</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>genuine</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>661a5d4bee4d349db0518761</td>\n",
       "      <td>qVc8ODYU5SZjKXVBgXdI7w</td>\n",
       "      <td>585</td>\n",
       "      <td>01vN0q6aMlFio6HAjLZz7Q</td>\n",
       "      <td>I loved everything about this lovely train sta...</td>\n",
       "      <td>2009-05-01 02:00:03</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>0.598016</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>genuine</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661a5d4bee4d349db0518762</td>\n",
       "      <td>j14WgRoU_-2ZE1aw1dXrJg</td>\n",
       "      <td>4333</td>\n",
       "      <td>iMpqDa0Oyukiw5406KYNRw</td>\n",
       "      <td>The Praline Connection makes a mean po' boy.  ...</td>\n",
       "      <td>2010-10-11 19:24:56</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024826</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>fake</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>661a5d4bee4d349db0518763</td>\n",
       "      <td>j14WgRoU_-2ZE1aw1dXrJg</td>\n",
       "      <td>4333</td>\n",
       "      <td>3pCHQ8YHkuaZFFAEz7pz6A</td>\n",
       "      <td>We walked over to Tennessee Brew Works, one of...</td>\n",
       "      <td>2018-10-23 14:24:32</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0.144998</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>genuine</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>661a5d4bee4d349db0518764</td>\n",
       "      <td>j14WgRoU_-2ZE1aw1dXrJg</td>\n",
       "      <td>4333</td>\n",
       "      <td>FvTfqugdlzQSvFB1RAafNQ</td>\n",
       "      <td>Logan Circle, also known as Logan Square, is a...</td>\n",
       "      <td>2019-08-04 12:29:33</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140657</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>genuine</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id                 user_id  review_count  \\\n",
       "0  661a5d4bee4d349db0518760  qVc8ODYU5SZjKXVBgXdI7w           585   \n",
       "1  661a5d4bee4d349db0518761  qVc8ODYU5SZjKXVBgXdI7w           585   \n",
       "2  661a5d4bee4d349db0518762  j14WgRoU_-2ZE1aw1dXrJg          4333   \n",
       "3  661a5d4bee4d349db0518763  j14WgRoU_-2ZE1aw1dXrJg          4333   \n",
       "4  661a5d4bee4d349db0518764  j14WgRoU_-2ZE1aw1dXrJg          4333   \n",
       "\n",
       "                review_id                                               text  \\\n",
       "0  Egy2a4qZeXGr2aY6KMxxbg  Remarkable food with beach access for the whol...   \n",
       "1  01vN0q6aMlFio6HAjLZz7Q  I loved everything about this lovely train sta...   \n",
       "2  iMpqDa0Oyukiw5406KYNRw  The Praline Connection makes a mean po' boy.  ...   \n",
       "3  3pCHQ8YHkuaZFFAEz7pz6A  We walked over to Tennessee Brew Works, one of...   \n",
       "4  FvTfqugdlzQSvFB1RAafNQ  Logan Circle, also known as Logan Square, is a...   \n",
       "\n",
       "                  date  stars  useful  funny  sentiment_score  ...  \\\n",
       "0  2017-05-09 23:21:36      5       0      0         0.405455  ...   \n",
       "1  2009-05-01 02:00:03      5      30      7         0.598016  ...   \n",
       "2  2010-10-11 19:24:56      3       3      1         0.024826  ...   \n",
       "3  2018-10-23 14:24:32      4      12      5         0.144998  ...   \n",
       "4  2019-08-04 12:29:33      4       5      0         0.140657  ...   \n",
       "\n",
       "   rule5_threshold  rule2_threshold  rule4_threshold  rule3_threshold  \\\n",
       "0                0                0                1                1   \n",
       "1                1                0                1                1   \n",
       "2                1                1                1                1   \n",
       "3                1                1                1                1   \n",
       "4                1                1                1                1   \n",
       "\n",
       "   similarity_score  rule6_threshold  count_ones   target funny_threshold  \\\n",
       "0               1.0                1           4  genuine               0   \n",
       "1               1.0                1           5  genuine               1   \n",
       "2               1.0                1           5     fake               1   \n",
       "3               1.0                1           6  genuine               1   \n",
       "4               1.0                1           6  genuine               0   \n",
       "\n",
       "   star_threshold  \n",
       "0               1  \n",
       "1               1  \n",
       "2               0  \n",
       "3               1  \n",
       "4               1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('final_dataset.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'user_id', 'review_count', 'review_id', 'text', 'date', 'stars',\n",
       "       'useful', 'funny', 'sentiment_score', 'rule1_threshold',\n",
       "       'rule5_threshold', 'rule2_threshold', 'rule4_threshold',\n",
       "       'rule3_threshold', 'similarity_score', 'rule6_threshold', 'count_ones',\n",
       "       'target', 'funny_threshold', 'star_threshold'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "# Drop the remaining columns\n",
    "columns_to_keep = ['user_id', 'review_id', 'text', 'date', 'stars', 'useful', 'funny', 'target']\n",
    "df.drop(columns=df.columns.difference(columns_to_keep), inplace=True)\n",
    "\n",
    "df.to_csv('final_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'review_id', 'text', 'date', 'stars', 'useful', 'funny',\n",
       "       'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7817994762439207\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       1.00      0.00      0.00      4667\n",
      "     genuine       0.78      1.00      0.88     16717\n",
      "\n",
      "    accuracy                           0.78     21384\n",
      "   macro avg       0.89      0.50      0.44     21384\n",
      "weighted avg       0.83      0.78      0.69     21384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Split data into features (text) and target\n",
    "X = data['text']\n",
    "y = data['target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Convert text data to TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the target variable on the test set\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate other metrics\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Class label Training(Naives Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7818930041152263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       1.00      0.00      0.00      4667\n",
      "     genuine       0.78      1.00      0.88     16717\n",
      "\n",
      "    accuracy                           0.78     21384\n",
      "   macro avg       0.89      0.50      0.44     21384\n",
      "weighted avg       0.83      0.78      0.69     21384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Custom transformer to select specific attributes\n",
    "class AttributeSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute):\n",
    "        self.attribute = attribute\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute]\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data[['text', 'useful', 'funny']]\n",
    "y = data['target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('selector', AttributeSelector(attribute='text')),\n",
    "                ('vectorizer', TfidfVectorizer())\n",
    "            ])),\n",
    "            ('numeric_pipeline', Pipeline([\n",
    "                ('selector', AttributeSelector(attribute=['useful', 'funny']))\n",
    "            ]))\n",
    "        ]\n",
    "    )),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate other metrics\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Single label training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8228582117471006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.65      0.40      0.50      4667\n",
      "     genuine       0.85      0.94      0.89     16717\n",
      "\n",
      "    accuracy                           0.82     21384\n",
      "   macro avg       0.75      0.67      0.69     21384\n",
      "weighted avg       0.81      0.82      0.81     21384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Split data into features (text) and target\n",
    "X = data['text']\n",
    "y = data['target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', HashingVectorizer(n_features=2**18)),\n",
    "    ('classifier', LinearSVC())\n",
    "])\n",
    "\n",
    "# Train the classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate other metrics\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Multi label Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8267396184062851\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.66      0.42      0.51      4667\n",
      "     genuine       0.85      0.94      0.89     16717\n",
      "\n",
      "    accuracy                           0.83     21384\n",
      "   macro avg       0.76      0.68      0.70     21384\n",
      "weighted avg       0.81      0.83      0.81     21384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Custom transformer to select specific attributes\n",
    "class AttributeSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute):\n",
    "        self.attribute = attribute\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute]\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data[['text', 'useful', 'funny']]\n",
    "y = data['target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('selector', AttributeSelector(attribute='text')),\n",
    "                ('vectorizer', HashingVectorizer(n_features=2**18))\n",
    "            ])),\n",
    "            ('numeric_pipeline', Pipeline([\n",
    "                ('selector', AttributeSelector(attribute=['useful', 'funny']))\n",
    "            ]))\n",
    "        ]\n",
    "    )),\n",
    "    ('classifier', LinearSVC())\n",
    "])\n",
    "\n",
    "# Train the classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate other metrics\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN SINGLE CLASS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 133ms/step - accuracy: 0.9918 - loss: 0.0454 - val_accuracy: 1.0000 - val_loss: 1.3637e-06\n",
      "Epoch 2/10\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 126ms/step - accuracy: 1.0000 - loss: 8.8025e-07 - val_accuracy: 1.0000 - val_loss: 3.1007e-07\n",
      "Epoch 3/10\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 2.2173e-07 - val_accuracy: 1.0000 - val_loss: 1.2307e-07\n",
      "Epoch 4/10\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 123ms/step - accuracy: 1.0000 - loss: 7.7338e-08 - val_accuracy: 1.0000 - val_loss: 5.8558e-08\n",
      "Epoch 5/10\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 4.9412e-08 - val_accuracy: 1.0000 - val_loss: 3.1262e-08\n",
      "Epoch 6/10\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 123ms/step - accuracy: 1.0000 - loss: 2.6192e-08 - val_accuracy: 1.0000 - val_loss: 1.7782e-08\n",
      "Epoch 7/10\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 125ms/step - accuracy: 1.0000 - loss: 1.3342e-08 - val_accuracy: 1.0000 - val_loss: 1.0473e-08\n",
      "Epoch 8/10\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 7.5246e-09 - val_accuracy: 1.0000 - val_loss: 6.3717e-09\n",
      "Epoch 9/10\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 4.5564e-09 - val_accuracy: 1.0000 - val_loss: 3.9870e-09\n",
      "Epoch 10/10\n",
      "\u001b[1m903/903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 123ms/step - accuracy: 1.0000 - loss: 3.1640e-09 - val_accuracy: 1.0000 - val_loss: 2.5893e-09\n",
      "\u001b[1m669/669\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     21384\n",
      "\n",
      "    accuracy                           1.00     21384\n",
      "   macro avg       1.00      1.00      1.00     21384\n",
      "weighted avg       1.00      1.00      1.00     21384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Split data into features (text) and target\n",
    "X = data['text']\n",
    "y = data['target']\n",
    "\n",
    "# Convert string labels to binary labels\n",
    "y_binary = (y == 'positive').astype(int)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_length = 100  # adjust this based on your data\n",
    "X_padded = pad_sequences(X_seq, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_binary, test_size=0.4, random_state=42)\n",
    "\n",
    "# Define the CNN model\n",
    "embedding_dim = 100  # adjust this based on your data\n",
    "vocab_size = len(tokenizer.word_index) + 1  # add 1 for padding token\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate other metrics\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
