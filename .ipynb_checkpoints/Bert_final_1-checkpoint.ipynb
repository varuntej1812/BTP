{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db228c15-c87b-457c-96b2-471962a95a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc035989-4407-454e-b1f3-ae970af09ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_dataset.csv')\n",
    "df_sampled = df\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df_sampled['text']\n",
    "y = df_sampled['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c7875d-dc65-4e1d-924b-4a766cf190c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer.batch_encode_plus(X_train.tolist(), \n",
    "                                              add_special_tokens=True, \n",
    "                                              max_length=128,  # Reduced max_length for efficiency\n",
    "                                              padding='max_length', \n",
    "                                              truncation=True, \n",
    "                                              return_attention_mask=True, \n",
    "                                              return_tensors='pt')\n",
    "\n",
    "test_encodings = tokenizer.batch_encode_plus(X_test.tolist(), \n",
    "                                             add_special_tokens=True, \n",
    "                                             max_length=128,  # Reduced max_length for efficiency\n",
    "                                             padding='max_length', \n",
    "                                             truncation=True, \n",
    "                                             return_attention_mask=True, \n",
    "                                             return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d479829-f068-4188-a2d6-5b5d86121c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'fake': 0, 'genuine': 1}\n",
    "y_train_num = [label_mapping[label] for label in y_train]\n",
    "y_test_num = [label_mapping[label] for label in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845177e3-6cc5-4c21-b212-4248c7ded6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = {k: v.clone().detach() for k, v in encodings.items()}\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f0feb5-9137-4e66-8512-8500debf1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = YelpDataset(train_encodings, y_train_num)\n",
    "test_dataset = YelpDataset(test_encodings, y_test_num)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Reduced batch_size for smaller data\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8371ee40-f3de-4329-8431-4d238cb4fb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f9138e-68a1-46e8-9e20-8a9acfaf17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch in loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa66c677-7ec7-49a3-91d5-101544c80496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    return predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3117f91d-8a72-4709-ba3a-1baf27c2980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c39b95e-39fb-434d-b824-128cdf16b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeefe4f-2598-4ffb-9c86-4f9b7f07abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):  # Reduced number of epochs for the smaller dataset\n",
    "    avg_loss, accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70865bb6-8053-4c73-b520-1de6aea63b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, true_labels = evaluate(model, device, test_loader)\n",
    "test_accuracy = accuracy_score(y_test_num, predictions)\n",
    "print(f'Final Test Accuracy: {test_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
