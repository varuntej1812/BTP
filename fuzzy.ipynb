{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUZZY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Reading the input dataset\n",
    "df_full = pd.read_csv(\"balanced_dataset.csv\")\n",
    "\n",
    "# Extracting features\n",
    "columns = list(df_full.columns)\n",
    "features = columns[:-1]  # All columns except 'target'\n",
    "class_labels = list(df_full['target'])  # Fake or Genuine\n",
    "df = df_full[features]\n",
    "\n",
    "# Convert text reviews to numerical vectors using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=500)  # Limit to top 500 features\n",
    "X_text = vectorizer.fit_transform(df['text']).toarray()\n",
    "\n",
    "# Combine text features with numerical ones (stars, useful, funny)\n",
    "df_numeric = df[['stars', 'useful', 'funny']].values\n",
    "df_combined = np.hstack((X_text, df_numeric))\n",
    "\n",
    "# Update number of data points and attributes\n",
    "num_attr = df_combined.shape[1]\n",
    "n = df_combined.shape[0]\n",
    "\n",
    "# Number of clusters and Fuzzy parameter\n",
    "k = 2\n",
    "m = 2.0\n",
    "MAX_ITER = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(cluster_labels, class_labels):\n",
    "    tp = [0, 0]\n",
    "    tn = [0, 0]\n",
    "    fp = [0, 0]\n",
    "    fn = [0, 0]\n",
    "    \n",
    "    # Mapping \"fake\" to 0 and \"genuine\" to 1\n",
    "    for i in range(len(df)):\n",
    "        # Cluster 0 -> Fake, Cluster 1 -> Genuine\n",
    "        if cluster_labels[i] == 0 and class_labels[i] == 'fake':\n",
    "            tp[0] += 1\n",
    "        if cluster_labels[i] == 1 and class_labels[i] == 'genuine':\n",
    "            tn[0] += 1\n",
    "        if cluster_labels[i] == 0 and class_labels[i] == 'genuine':\n",
    "            fp[0] += 1\n",
    "        if cluster_labels[i] == 1 and class_labels[i] == 'fake':\n",
    "            fn[0] += 1\n",
    "        \n",
    "        # Inverting the clusters to account for different possibilities\n",
    "        if cluster_labels[i] == 1 and class_labels[i] == 'fake':\n",
    "            tp[1] += 1\n",
    "        if cluster_labels[i] == 0 and class_labels[i] == 'genuine':\n",
    "            tn[1] += 1\n",
    "        if cluster_labels[i] == 1 and class_labels[i] == 'genuine':\n",
    "            fp[1] += 1\n",
    "        if cluster_labels[i] == 0 and class_labels[i] == 'fake':\n",
    "            fn[1] += 1\n",
    "    \n",
    "    a0 = (tp[0] + tn[0]) / (tp[0] + tn[0] + fn[0] + fp[0])\n",
    "    a1 = (tp[1] + tn[1]) / (tp[1] + tn[1] + fn[1] + fp[1])\n",
    "    p0 = tp[0] / (tp[0] + fp[0])\n",
    "    p1 = tp[1] / (tp[1] + fp[1])\n",
    "    r0 = tp[0] / (tp[0] + fn[0])\n",
    "    r1 = tp[1] / (tp[1] + fn[1])\n",
    "    \n",
    "    accuracy = [a0 * 100, a1 * 100]\n",
    "    precision = [p0 * 100, p1 * 100]\n",
    "    recall = [r0 * 100, r1 * 100]\n",
    "    \n",
    "    return accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeMembershipMatrix():\n",
    "    membership_mat = []\n",
    "    for i in range(n):\n",
    "        random_nums = [random.random() for _ in range(k)]\n",
    "        summation = sum(random_nums)\n",
    "        temp_list = [x/summation for x in random_nums]\n",
    "        membership_mat.append(temp_list)\n",
    "    return membership_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateClusterCenter(membership_mat):\n",
    "    cluster_mem_val = list(zip(*membership_mat))  # Convert zip to list\n",
    "    cluster_centers = []\n",
    "    for j in range(k):\n",
    "        x = list(cluster_mem_val[j])\n",
    "        xraised = [e ** m for e in x]\n",
    "        denominator = sum(xraised)\n",
    "        temp_num = []\n",
    "        for i in range(n):\n",
    "            data_point = df_combined[i]\n",
    "            prod = [xraised[i] * val for val in data_point]\n",
    "            temp_num.append(prod)\n",
    "        numerator = map(sum, zip(*temp_num))\n",
    "        center = [z / denominator for z in numerator]\n",
    "        cluster_centers.append(center)\n",
    "    return cluster_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateMembershipValue(membership_mat, cluster_centers):\n",
    "    p = float(2 / (m - 1))\n",
    "    for i in range(n):\n",
    "        x = df_combined[i]\n",
    "        distances = [np.linalg.norm(list(map(operator.sub, x, cluster_centers[j]))) for j in range(k)]\n",
    "        for j in range(k):\n",
    "            den = sum([math.pow(float(distances[j]/distances[c]), p) for c in range(k)])\n",
    "            membership_mat[i][j] = float(1 / den)\n",
    "    return membership_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzyCMeansClustering():\n",
    "    membership_mat = initializeMembershipMatrix()\n",
    "    curr = 0\n",
    "    while curr <= MAX_ITER:\n",
    "        cluster_centers = calculateClusterCenter(membership_mat)\n",
    "        membership_mat = updateMembershipValue(membership_mat, cluster_centers)\n",
    "        cluster_labels = getClusters(membership_mat)\n",
    "        curr += 1\n",
    "    return cluster_labels, cluster_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClusters(membership_mat):\n",
    "    cluster_labels = []\n",
    "    for i in range(n):\n",
    "        max_val, idx = max((val, idx) for (idx, val) in enumerate(membership_mat[i]))\n",
    "        cluster_labels.append(idx)\n",
    "    return cluster_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [[0.009365439691531625, 0.003517446762079141, 0.005405418184697359, 0.006119530071535375, 0.006034246808070287, 0.005016838845897213, 0.006007228612151854, 0.005206469787121787, 0.009445093025097913, 0.004067232971696112, 0.005110299191383402, 0.013432210058172288, 0.0046885323816468925, 0.016288651486366015, 0.0044160377089238895, 0.006703903252992848, 0.008173469301189451, 0.005142663663996918, 0.011021311210432037, 0.005067434610665444, 0.004035467074588393, 0.008016349808255436, 0.009083039389964496, 0.01172194562643956, 0.005745562534551379, 0.015287373679503688, 0.02248957026623938, 0.0045637935450727855, 0.00377997935461962, 0.006142944314821672, 0.005100808927935834, 0.005303647735744558, 0.007383928362944866, 0.01610914518539961, 0.006048227842365632, 0.00408681916352764, 0.02121975664068691, 0.019672405310614982, 0.012570748741438936, 0.016283263495739782, 0.004433292009787591, 0.004354595896797374, 0.0058208205535694325, 0.0046337929397956895, 0.009947582432467307, 0.012290780365899237, 0.006259191045911985, 0.004964271291843916, 0.0065545997308576595, 0.005323816037674633, 0.013715278670410162, 0.005789819634421696, 0.006367986812452265, 0.007951758451814627, 0.004012190828480186, 0.005568281610350414, 0.005554707363299277, 0.005013260110223687, 0.005593336352447179, 0.01744972771008162, 0.006070358595840132, 0.005528196350857159, 0.004521931011380369, 0.004049683751394784, 0.009109507186259656, 0.00977729647527876, 0.016793276431323376, 0.020998565083277274, 0.004813882203104734, 0.006351065775012616, 0.006212671646038583, 0.005581087057960247, 0.010075612753662777, 0.010853588426814442, 0.00749941857770555, 0.01384585903403264, 0.0074961986249225026, 0.017213912467097477, 0.005507644765513826, 0.007589199250836623, 0.003984301425148414, 0.006662074819131591, 0.00977566172983642, 0.006055309455533144, 0.005038551774903377, 0.007664686625168793, 0.006034984348457182, 0.0060040248393254106, 0.009931399222842226, 0.0037947029739591903, 0.00515968706602634, 0.005304834687126491, 0.004384783375349618, 0.006134002157934385, 0.004230639908089115, 0.004555792971609523, 0.005076530692603703, 0.015252099030341486, 0.005240986080843265, 0.0061042556822637995, 0.012943357719591196, 0.006749944751977033, 0.006713234094366332, 0.017120196114432898, 0.01663069073910144, 0.00598651388000599, 0.01665551243660413, 0.018444780869304903, 0.010162380711360943, 0.00681587642071525, 0.01226558623391887, 0.006983538694071777, 0.009127486537006973, 0.007885652778730497, 0.006745483670243953, 0.007357670051165611, 0.004530559966443515, 0.004118621384576313, 0.02384813233297861, 0.006677947193873184, 0.004841376451014442, 0.011550925971259179, 0.013794160866767383, 0.006037158535534004, 0.00569307060893215, 0.0050347367753315565, 0.0056626386872000705, 0.014705082972518916, 0.00750830260191876, 0.004513869557760207, 0.005181613121049962, 0.006901246053306124, 0.004563619984921819, 0.008241761922057174, 0.00805997682977965, 0.004304662472122302, 0.007926109294949185, 0.004285950247252181, 0.003863604364950053, 0.009774193793150678, 0.006441827051608973, 0.004375046274213971, 0.007338424353191426, 0.013764583477398293, 0.005835553693043286, 0.0047669672399737574, 0.005752192205811508, 0.006420356135636061, 0.007569031825315914, 0.006767886482330819, 0.007753226928054878, 0.008611678700397384, 0.011203534262814991, 0.011514795035517946, 0.006307248330603207, 0.005103139779948569, 0.007403347884129073, 0.008559563821432456, 0.009761056055844108, 0.006117520113358272, 0.00501991832101666, 0.0510052299909937, 0.010419760789955334, 0.006152224769856246, 0.014651962468625913, 0.004473966501529133, 0.0111714325275385, 0.010566643846713328, 0.020424977636560433, 0.010230018689017871, 0.01248643770569527, 0.010048190144744555, 0.004996500719462123, 0.0036744897108444024, 0.006437517053091424, 0.005482587486792758, 0.008550818237052773, 0.004541002773052922, 0.004189403054597487, 0.004171066651627131, 0.015249593950116852, 0.05227576336650677, 0.02300601917328715, 0.04253451050016616, 0.0048254409825557575, 0.004872236172866744, 0.0066458592281035955, 0.005806324717643656, 0.005610125153442751, 0.004661202070665267, 0.0043579651409111275, 0.008290487450146607, 0.004377390381933021, 0.01100275546083148, 0.008718842529840582, 0.0049686141478618015, 0.006938708412950746, 0.003938494522222348, 0.004443871013778319, 0.005351557499253023, 0.006822413528506281, 0.008165610855116514, 0.005561766486593111, 0.005804998862413462, 0.011608040745931471, 0.012599177660313206, 0.010288451646584611, 0.010088564084401648, 0.006750569020195545, 0.007834347950487848, 0.009364695454508201, 0.006016102390957009, 0.009277595025996017, 0.0034319099519944797, 0.005556055997887864, 0.009098264074716938, 0.00584819332486982, 0.004760175413420008, 0.008587528160861897, 0.005678821759492879, 0.009069463730924687, 0.005533438495675087, 0.03757518783509431, 0.003837569222291851, 0.0056707244429611385, 0.010417041267299311, 0.004772986462877714, 0.0035372707172706295, 0.01596425138663588, 0.009316725872722633, 0.006295563045782217, 0.0051821569976474155, 0.004961666482582474, 0.008102237123641933, 0.00703253289222066, 0.0042542453342062705, 0.004493334499214897, 0.03909992378679438, 0.006258580185490433, 0.009232699190537541, 0.005254632864381684, 0.022838312467790316, 0.00732588265260294, 0.015101629042458235, 0.008691938285562488, 0.004509392522173542, 0.015820523902486627, 0.012170860855347545, 0.008051116590374925, 0.007182830394841642, 0.011413802639934946, 0.005195161778629784, 0.014289877422751207, 0.007611743183583978, 0.004360297843904105, 0.022456582095175603, 0.007623569897501752, 0.0168677240610116, 0.0041786049680851015, 0.016126880817192876, 0.007588935148945846, 0.004347315446502174, 0.005250334908546297, 0.003426290060369806, 0.0037404472940574065, 0.009910805445863077, 0.012365711345924138, 0.004695934346531845, 0.009548374666306012, 0.01863154872434343, 0.006251606121487447, 0.00458162119590648, 0.010904090483060786, 0.005622603507313543, 0.005209042217902665, 0.009083921760042303, 0.0056388124023976655, 0.012211915586316952, 0.006494206390743821, 0.0056064951038686806, 0.01635760142663652, 0.024360850099478408, 0.025423995453365253, 0.0045421144727307485, 0.017506808791846257, 0.00701345451788226, 0.005742844894300032, 0.00501376505556079, 0.0034722721905119515, 0.0034211541484849882, 0.024150784378229092, 0.006184615232700001, 0.005694369008944493, 0.008331956684121776, 0.0057909833152009595, 0.0059352680680867945, 0.013270035379816785, 0.007655516146209908, 0.010839129834173868, 0.0035772950227589027, 0.009004761894492534, 0.0044342724380456965, 0.008605808012299761, 0.018100726301661495, 0.018579355195947934, 0.004085274349245797, 0.005653064163827033, 0.009408070538203958, 0.0074472443572005526, 0.004266481746180041, 0.005278050682311306, 0.004594313814303088, 0.009112790433268087, 0.005738561939956928, 0.0045716110026093595, 0.004653983610874763, 0.004643895021394478, 0.006191154575063419, 0.019225251463035706, 0.009178489875572838, 0.004371184567426461, 0.005842276152502066, 0.008592385151269501, 0.005233949980762827, 0.01887974511364127, 0.04993102497391607, 0.010966612841359703, 0.005991773611689868, 0.005735941092361318, 0.006430765661435196, 0.005385009232336154, 0.008026166354655617, 0.003932985454901167, 0.005719642425165227, 0.005260424970891966, 0.025034134624627523, 0.011889525514150782, 0.005395906870201383, 0.013538321236868956, 0.010913876880064137, 0.004116273116801534, 0.009773093548089312, 0.008857705942050789, 0.005055357651232255, 0.009979728528670096, 0.0037107903458013123, 0.0068200285226211865, 0.03335365963302025, 0.005982073893481464, 0.0053692178029787875, 0.010016911684062303, 0.00601863115391274, 0.005688181745964817, 0.005552063368043437, 0.004052010952522583, 0.01819595631749108, 0.006931644305771798, 0.005920344580676117, 0.008492564371206145, 0.005925364592010134, 0.008809839840212075, 0.014728161825090536, 0.006785730552297061, 0.006633579666489994, 0.013987690459863521, 0.005053610493596951, 0.005367019785326495, 0.011384840323675328, 0.014668117598976214, 0.004121858843138489, 0.01295278283304711, 0.006182049400480395, 0.00518922979913125, 0.005333077492797103, 0.014351501313520955, 0.003973485104649686, 0.0053026970773928555, 0.014432697709603414, 0.0052126222475263, 0.004664299364894319, 0.007215639860962805, 0.005502399285643132, 0.004190412786402535, 0.014676463360362348, 0.004106941594579967, 0.004957074830012331, 0.00713090765883063, 0.008368221766886027, 0.03166045465047032, 0.004151635102876902, 0.008079332652069171, 0.004296266785087823, 0.010130401279339124, 0.004375838619731204, 0.007199278588302923, 0.004315817364752048, 0.005891501675893033, 0.006842785970573415, 0.014710676813444875, 0.00540625345417623, 0.004713036245650754, 0.003531350876868394, 0.009181198704848433, 0.004697492837663768, 0.006503132740673868, 0.010919365238168955, 0.008183389056456237, 0.010987938552549734, 0.007141758877710449, 0.019612381119146344, 0.005173232863242521, 0.008060337860608332, 0.012747486369094965, 0.0040059815743312705, 0.004973169613841297, 0.006509739069476623, 0.006967700035466237, 0.008371696695771879, 0.005994560078313074, 0.01388888567262688, 0.009748526033620595, 0.007689776443302054, 0.006406124091718553, 0.005531037212152708, 0.011310018364349443, 0.014401086172786895, 0.011523604641645946, 0.010851606564945413, 0.012242937151298433, 0.007735126574415697, 0.004501510212391393, 0.006692471420991511, 0.01017283087634959, 0.007312089477108971, 0.0119002544784004, 0.0064739208568818755, 0.005285505222821765, 0.006669733529022559, 0.012317371866083305, 0.010792117899629094, 0.01758441814805813, 0.008758730160014652, 0.030776982143161546, 0.012760842151233088, 0.003839464422297501, 0.003625810683577998, 0.0046787856206616995, 0.007335903049559485, 0.004012121873666414, 0.010391842392539914, 0.004946503713630333, 0.0047229746845112615, 0.008008100402554988, 0.011288851397349932, 0.006018401302761352, 0.018569258337688157, 0.006451330918029735, 0.004236536219124398, 0.004204123993418658, 0.004002920110834227, 0.00707408754049999, 0.008923481885359803, 0.009287379998872478, 0.005515486062929283, 0.026188343397889183, 0.01017620887878504, 0.015019961329399392, 0.003950929785483951, 0.004990557600337963, 0.006081888365200008, 0.006367620306014321, 0.007318009640668218, 0.0052664673411680855, 0.004677497492543265, 0.015028966084085726, 0.008861558966244626, 0.004682639267317359, 0.014235126651704021, 0.007381378575266432, 0.016561131526871418, 0.006142949524210907, 0.005082596314087798, 0.017430725942754254, 0.00392267737362371, 0.004747549186333251, 0.00464746342043021, 0.009707487040506264, 0.00569768649019805, 0.006414528356297315, 0.007277493450164772, 0.005857512006360786, 0.01060571413339159, 0.004718799126876573, 0.012052329839242001, 0.006572858739828723, 0.006563996631037907, 0.006259656420667483, 0.008860856634604664, 0.004895498130798413, 0.006359390031265574, 3.3819729102034097, 1.1189884579695994, 0.42721312021506513], [0.013386484536506411, 0.005898777538293097, 0.007151711215130219, 0.00883096770132764, 0.008735537986796075, 0.008551779713333183, 0.009478771130314163, 0.006467874527458274, 0.014843759412163872, 0.005508214427824673, 0.006765170249987432, 0.010717188925884592, 0.004116595735936006, 0.01803039010597818, 0.007117783401890758, 0.010442571432506034, 0.014474809063510693, 0.006027493402839868, 0.006458478194940736, 0.004464202972605421, 0.007171431015786214, 0.004779278154259924, 0.013481424127243104, 0.009861701163510707, 0.008023215049446801, 0.013926581095760705, 0.02458346826885768, 0.00499210189660299, 0.005104052126760548, 0.005098329136930282, 0.006039558729475739, 0.008466315302650773, 0.007951933894468296, 0.015318498790504129, 0.005208525422594246, 0.006959157241875548, 0.01966039778307507, 0.021744416814735122, 0.015301385196217201, 0.015599781226529066, 0.005458442921909439, 0.0075009850072876175, 0.004170378110289365, 0.006361895503491548, 0.011417981318256304, 0.010535239389953554, 0.008578457990463926, 0.007778504305054052, 0.00431158117495492, 0.004827329412176507, 0.014826959720694106, 0.00603998515335634, 0.011982851585495051, 0.006848696071091965, 0.005310947574032222, 0.00819871011603591, 0.0050655750261012255, 0.009332168179563458, 0.010568445626256038, 0.018948114260672395, 0.0127137695158881, 0.00933679380643914, 0.0056939504197599435, 0.005668601156838472, 0.008379159408821477, 0.014874719191302125, 0.018345438962662258, 0.01937548958300715, 0.004361174080519451, 0.0059692775256282955, 0.007636370012225796, 0.005604775847883712, 0.01346021348153941, 0.010434791293995337, 0.009328426951375786, 0.015069695883613548, 0.008370366375943277, 0.0212696013091734, 0.00812956970784852, 0.00881441349060817, 0.005687557547306305, 0.006906443803056663, 0.010570945902663323, 0.008651851394664883, 0.008412666977826916, 0.009799568904778268, 0.0087450809288847, 0.007033051291245377, 0.009669881069776981, 0.0059435002051075785, 0.005709590653215529, 0.004329572378421864, 0.00491004338934822, 0.00992933917603268, 0.005971329629663542, 0.007008692442451986, 0.005607026280090266, 0.022662891589795654, 0.00904165857942764, 0.008328336868441411, 0.009766922513240383, 0.00972523181634607, 0.0062478718825671855, 0.014604007886385491, 0.015475953397258045, 0.006209708552994924, 0.02464390367988977, 0.02447327917153282, 0.013617364347780981, 0.00918082917434181, 0.012450827151964808, 0.005105725378677771, 0.010776186122699063, 0.007476401607407902, 0.008789766432036689, 0.009730341625293258, 0.007037160710913617, 0.007312048106239017, 0.03340818837905652, 0.009402692608509929, 0.004690181725473433, 0.013670140145640925, 0.012567541055107101, 0.008375126089188029, 0.005995078505765609, 0.00662699911033293, 0.008618705896442345, 0.016117795447110936, 0.007937609211198189, 0.005553193789932928, 0.0059618667785060865, 0.0115616211092461, 0.0070872266623582154, 0.00982756434086915, 0.00805995557092541, 0.005777083304807197, 0.008469588641149875, 0.005448730182646125, 0.006060128063344192, 0.006024459271391518, 0.006974620937890079, 0.004486704845690376, 0.007605174898048067, 0.01952185332469764, 0.008062452366709983, 0.006501437819781157, 0.008621173045136207, 0.00791670488461209, 0.00786651047083128, 0.005650253781231082, 0.00949508061325626, 0.006992461024979707, 0.01063028482389119, 0.014726764709429766, 0.008812539989198355, 0.009811198111239042, 0.009219674458107244, 0.009917476999651103, 0.010482630718958498, 0.0062031803139318675, 0.007905632858321165, 0.040828025490634856, 0.013158578255619757, 0.006022270988511331, 0.014438764973086564, 0.005044774310709815, 0.012174996739635005, 0.012507502003000923, 0.01693069101851411, 0.011659585373601873, 0.011847899807021724, 0.010548029729669542, 0.00640820513473688, 0.0049350754987834075, 0.008081529777961104, 0.006732643020166403, 0.01118576924218902, 0.006079148584510462, 0.005422419943890062, 0.006780878016075896, 0.022058646318444015, 0.042642679775932946, 0.027168692925420405, 0.03135932290080242, 0.0076562630482650326, 0.005458068706171352, 0.008714164365374486, 0.008296836417780411, 0.009536159950430648, 0.006894549120818981, 0.008927183217176369, 0.01134132053292093, 0.0070665549822466625, 0.01317913304288292, 0.010344953397090148, 0.005817330035345036, 0.011080212043794118, 0.006394677939761939, 0.006824098958115785, 0.008552499134166567, 0.00663931703034567, 0.012414156816993423, 0.004420111974677448, 0.006036989986332993, 0.01569144438955975, 0.01613184440284309, 0.012449421314901184, 0.010967946018571643, 0.01102309074205712, 0.012782872839181916, 0.011734645661081438, 0.00660620755883861, 0.009062829246182975, 0.006944387161391885, 0.005368464834864374, 0.011878762051882076, 0.00850178017585003, 0.005564304845294163, 0.008713077760843608, 0.005484913588557203, 0.010722232370988202, 0.00742941286035074, 0.048186294154958766, 0.005283912484827711, 0.008029740256819157, 0.012605168505180288, 0.007504342309637356, 0.006257182429648673, 0.025388003937745228, 0.011322761507225677, 0.006559523365253517, 0.00982198796460393, 0.008402159193662, 0.012966845088051947, 0.011720890120633081, 0.007604772222646443, 0.006776249724311177, 0.05329413724729309, 0.006876912249265514, 0.01256214331720781, 0.007566151028677006, 0.02518883946372224, 0.009270344319071708, 0.022163696428497703, 0.010238005001781802, 0.006362645436159242, 0.014075950805746754, 0.015551538343762633, 0.012562124348612974, 0.011469367859362763, 0.014192504521988277, 0.007143742488022465, 0.01718760865098891, 0.00832585789082729, 0.005089741990849877, 0.021963275759358317, 0.006556204369620308, 0.015399205547241258, 0.0054708151822447084, 0.021446188214926824, 0.009846293343341328, 0.008331226884222201, 0.004196808400408491, 0.007106346680380021, 0.006299388694458278, 0.014078507304807631, 0.013526765574197243, 0.00761014062594481, 0.0108347298016394, 0.022112672908279684, 0.0040190131370592755, 0.00649130408887569, 0.015252146996191962, 0.009961701194246662, 0.006158271675238089, 0.010293404265025006, 0.006044442828083833, 0.01590955306072115, 0.009609085802341788, 0.004990054860947547, 0.023714766425797548, 0.03914636636856222, 0.02484840687110236, 0.008531599764119699, 0.019172287931797243, 0.010656768640449812, 0.010864048068137353, 0.007277160841579016, 0.00408680353142704, 0.009013027143974684, 0.036532007715591355, 0.008702644904778135, 0.010553043390073333, 0.014175099152045574, 0.00874666414319211, 0.010007148138226316, 0.011841552270213591, 0.007859537146505112, 0.014640402878782943, 0.005225939066815504, 0.012268313823254518, 0.004974138329388515, 0.008686120966388109, 0.021590941948058467, 0.017924330059358198, 0.004742863049092471, 0.006835804637675648, 0.0112810525021356, 0.005387326210592294, 0.009004063287244941, 0.004644521590973055, 0.00685475522898816, 0.011705535565081677, 0.007386984209427338, 0.005918379693270743, 0.003858201585563395, 0.004646929339887057, 0.011022289991452465, 0.027872574770981057, 0.009673906909544502, 0.00555550918912183, 0.009646835952751428, 0.0071751088791052915, 0.006919553140482315, 0.01736014350814213, 0.04982717830055025, 0.011693912937667746, 0.007696323000730966, 0.0063146212904180744, 0.0067209681611540865, 0.00949106390222373, 0.008668497841734235, 0.0038340747532458813, 0.004185295383805092, 0.005626600962881823, 0.021330224532527493, 0.013297043208873642, 0.005119790353048558, 0.013316109473384054, 0.012335608701563895, 0.005435607498300064, 0.011218690396471331, 0.00824914017364385, 0.0062875101090258375, 0.011037602161319531, 0.005677918024675667, 0.007849494840099511, 0.03343440161064263, 0.007590930424932713, 0.005023771898380037, 0.007334725815564508, 0.00790876097530075, 0.0076806419845727135, 0.005683099822652181, 0.0058677246111485185, 0.01999871443991778, 0.00810071575569587, 0.006847452902924706, 0.016078644224612245, 0.00867621157836898, 0.007717268366620667, 0.02028757713621089, 0.006283742084990946, 0.0062401255597824236, 0.02067643119354979, 0.007073043637146937, 0.0075999297814602984, 0.019703995285387566, 0.014060485485825565, 0.004638812504082012, 0.01199047319160662, 0.0062459874338404, 0.0063883895602504185, 0.006194521089442096, 0.015865941304047754, 0.004761714815058682, 0.008167058143693007, 0.02060696432153323, 0.004736845200869976, 0.005301559041984219, 0.006581248291400138, 0.007048806171697478, 0.007831008766562703, 0.011115856355653521, 0.006173334172721169, 0.006489149627965067, 0.011719719167021417, 0.00805763065719428, 0.026462177032295097, 0.007574803600328445, 0.01204203617194612, 0.006287753404367322, 0.008301621238510937, 0.004756018515369225, 0.007344775675745003, 0.005838117593728236, 0.006706220495527803, 0.0054071770724270106, 0.01733694846507231, 0.004641815546853239, 0.006639390830583464, 0.005765462620692709, 0.008307444785917795, 0.0044282271144082536, 0.009668543670119953, 0.010607209601843155, 0.007821888994962015, 0.011448646995815033, 0.0076599641283800896, 0.016832707834707864, 0.004344232230882272, 0.011585136155371697, 0.013953923720804935, 0.006720142845840664, 0.008500992085309824, 0.00851259638356061, 0.007113570262084221, 0.010877477156692711, 0.00535189195043669, 0.019381449674242456, 0.010502357641890522, 0.009779333083719132, 0.008956683298238104, 0.004104812930593249, 0.011934417708872071, 0.016399721759037682, 0.00975281221734128, 0.012347335700371869, 0.017276453483774525, 0.009759861728495061, 0.0038853067028899865, 0.005124154560980382, 0.011690304530572616, 0.007997802294888052, 0.009484495236554688, 0.008495503739746042, 0.010712382702544212, 0.005191706260565998, 0.01581620245967029, 0.017251683578933842, 0.0237814874457625, 0.011505033265460303, 0.038048751364967145, 0.015446361530319115, 0.004808771475965428, 0.00669913575520013, 0.006902177616565914, 0.014817881549813169, 0.005780800557524351, 0.014600542342867534, 0.006772160387564189, 0.004956697397418014, 0.00963571176145345, 0.011130793424919709, 0.006101535321055887, 0.019015845524664978, 0.009296950031409686, 0.005426980502741453, 0.005472457333476484, 0.004546332667421021, 0.011713088635272099, 0.012277836693590833, 0.008685494231116348, 0.0055593387410596325, 0.03145495985721831, 0.012368475763450982, 0.015694320511550078, 0.005477499850051716, 0.00493271422627279, 0.008515463962758936, 0.007708542972463892, 0.010551778693980286, 0.008341023736722582, 0.0063359244177282744, 0.02041844736789724, 0.013946511582448631, 0.005163620590017632, 0.01696916106398503, 0.010095714155839283, 0.023478240775124046, 0.00932916568119256, 0.006036914573391886, 0.019290974165736625, 0.0055528040752349074, 0.007491935803819889, 0.0071234168054735625, 0.01024929326772673, 0.0074734124139570245, 0.005681829907607689, 0.009885492557118796, 0.006800255066547559, 0.015615307463608785, 0.007787157508432261, 0.01077373669326633, 0.007937393103421203, 0.008154124392403815, 0.01059222258655374, 0.013746743123001235, 0.01030970769661028, 0.010891571344939989, 3.190641529991558, 7.3180273827428435, 3.5694624149305216]]\n",
      "Accuracy: [49.26183239253148, 50.73816760746852]\n",
      "Precision: [49.57363563402889, 52.74725274725275]\n",
      "Recall: [85.82718193660442, 14.172818063395571]\n"
     ]
    }
   ],
   "source": [
    "labels, centers = fuzzyCMeansClustering()\n",
    "\n",
    "# Print cluster centers\n",
    "print(\"Cluster Centers:\", centers)\n",
    "\n",
    "# Evaluate accuracy\n",
    "a, p, r = accuracy(labels, class_labels)\n",
    "print(f\"Accuracy: {a}\")\n",
    "print(f\"Precision: {p}\")\n",
    "print(f\"Recall: {r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUZZY 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUZZY 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\2659696080.py:38: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.937559704732957\n",
      "Precision: 0.9914171462011119\n",
      "Recall: 0.8827616152844117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('balanced_dataset.csv')\n",
    "    features = ['stars', 'useful', 'funny']  # Use relevant numerical features\n",
    "    X = df[features].values\n",
    "    y = df['target'].map({'genuine': 1, 'fake': 0}).values\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Fuzzy C-means clustering\n",
    "    fcm = FuzzyCMeans(n_clusters=2, m=2.0, max_iter=100, epsilon=0.01)\n",
    "    fcm.fit(X)\n",
    "    y_pred = fcm.predict()\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y, y_pred, target_names=['fake', 'genuine'])\n",
    "    print('\\nClassification Report:\\n', report)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUZZY 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\2016099517.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.937559704732957\n",
      "Precision: 0.9914171462011119\n",
      "Recall: 0.8827616152844117\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.89      0.99      0.94     11515\n",
      "     genuine       0.99      0.88      0.93     11515\n",
      "\n",
      "    accuracy                           0.94     23030\n",
      "   macro avg       0.94      0.94      0.94     23030\n",
      "weighted avg       0.94      0.94      0.94     23030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('balanced_dataset.csv')\n",
    "    features = ['stars', 'useful', 'funny']  # Use relevant numerical features\n",
    "    X = df[features].values\n",
    "    y = df['target'].map({'genuine': 1, 'fake': 0}).values\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Fuzzy C-means clustering\n",
    "    fcm = FuzzyCMeans(n_clusters=2, m=2.0, max_iter=100, epsilon=0.01)\n",
    "    fcm.fit(X)\n",
    "    y_pred = fcm.predict()\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y, y_pred, target_names=['fake', 'genuine'])\n",
    "    print('\\nClassification Report:\\n', report)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\1261665101.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\1261665101.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\1261665101.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\1261665101.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\1261665101.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\1261665101.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\1261665101.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\1261665101.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\1261665101.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\1261665101.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.5026\n",
      "Mean Precision: 0.5014\n",
      "Mean Recall: 0.5176\n",
      "Classification Report (last run):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.55      0.52      0.53     11515\n",
      "     genuine       0.54      0.58      0.56     11515\n",
      "\n",
      "    accuracy                           0.55     23030\n",
      "   macro avg       0.55      0.55      0.55     23030\n",
      "weighted avg       0.55      0.55      0.55     23030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "class FuzzyCMeans:\n",
    "    def __init__(self, n_clusters=2, m=2.0, max_iter=100, epsilon=0.01):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.m = m\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.u = np.random.rand(self.n_samples, self.n_clusters)\n",
    "        self.u = self.u / np.sum(self.u, axis=1, keepdims=True)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            self.centers = self._calculate_centers()\n",
    "            u_prev = self.u.copy()\n",
    "            self._update_membership()\n",
    "            if np.linalg.norm(self.u - u_prev) < self.epsilon:\n",
    "                break\n",
    "\n",
    "    def _calculate_centers(self):\n",
    "        centers = np.zeros((self.n_clusters, self.n_features))\n",
    "        for j in range(self.n_clusters):\n",
    "            num = np.sum((self.u[:, j] ** self.m)[:, np.newaxis] * self.X, axis=0)\n",
    "            denom = np.sum(self.u[:, j] ** self.m)\n",
    "            centers[j] = num / denom\n",
    "        return centers\n",
    "\n",
    "    def _update_membership(self):\n",
    "        for i in range(self.n_samples):\n",
    "            for j in range(self.n_clusters):\n",
    "                dist = np.linalg.norm(self.X[i] - self.centers[j])\n",
    "                self.u[i, j] = 1 / np.sum(\n",
    "                    (dist / np.linalg.norm(self.X[i] - self.centers[k])) ** (2 / (self.m - 1))\n",
    "                    for k in range(self.n_clusters)\n",
    "                )\n",
    "\n",
    "    def predict(self):\n",
    "        return np.argmax(self.u, axis=1)\n",
    "\n",
    "def evaluate_model(X, y, n_runs=10):\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    reports = []\n",
    "\n",
    "    for _ in range(n_runs):\n",
    "        fcm = FuzzyCMeans(n_clusters=2, m=2.0, max_iter=100, epsilon=0.01)\n",
    "        fcm.fit(X)\n",
    "        y_pred = fcm.predict()\n",
    "\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        precision = precision_score(y, y_pred)\n",
    "        recall = recall_score(y, y_pred)\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "        report = classification_report(y, y_pred, target_names=['fake', 'genuine'], zero_division=0)\n",
    "        reports.append(report)\n",
    "\n",
    "    return np.mean(accuracy_list), np.mean(precision_list), np.mean(recall_list), reports\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('balanced_dataset.csv')\n",
    "    \n",
    "    # Focus on the text feature only\n",
    "    X = df['text']\n",
    "    y = df['target'].map({'genuine': 1, 'fake': 0}).values\n",
    "\n",
    "    # Convert text to TF-IDF features with n-grams\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.9, min_df=5)\n",
    "    X_tfidf = vectorizer.fit_transform(X)  # Keep sparse matrix\n",
    "\n",
    "    # Apply TruncatedSVD to reduce dimensionality (works directly on sparse data)\n",
    "    svd = TruncatedSVD(n_components=100)  # You can try changing n_components\n",
    "    X_reduced = svd.fit_transform(X_tfidf)\n",
    "\n",
    "    # Evaluate model\n",
    "    mean_accuracy, mean_precision, mean_recall, reports = evaluate_model(X_reduced, y, n_runs=10)\n",
    "\n",
    "    print(f'Mean Accuracy: {mean_accuracy:.4f}')\n",
    "    print(f'Mean Precision: {mean_precision:.4f}')\n",
    "    print(f'Mean Recall: {mean_recall:.4f}')\n",
    "\n",
    "    # Print the classification report from the last run\n",
    "    print(\"Classification Report (last run):\")\n",
    "    print(reports[-1])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\3042425601.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Mean Accuracy: 0.5210\n",
      "Best Mean Precision: 0.5188\n",
      "Best Mean Recall: 0.5446\n",
      "Best m: 2.0\n",
      "Best Threshold: 0.5\n",
      "Best Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.53      0.51      0.52     11515\n",
      "     genuine       0.53      0.55      0.54     11515\n",
      "\n",
      "    accuracy                           0.53     23030\n",
      "   macro avg       0.53      0.53      0.53     23030\n",
      "weighted avg       0.53      0.53      0.53     23030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "class FuzzyCMeans:\n",
    "    def __init__(self, n_clusters=2, m=2.0, max_iter=150, epsilon=0.01):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.m = m\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.u = np.random.rand(self.n_samples, self.n_clusters)\n",
    "        self.u = self.u / np.sum(self.u, axis=1, keepdims=True)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            self.centers = self._calculate_centers()\n",
    "            u_prev = self.u.copy()\n",
    "            self._update_membership()\n",
    "            if np.linalg.norm(self.u - u_prev) < self.epsilon:\n",
    "                break\n",
    "\n",
    "    def _calculate_centers(self):\n",
    "        centers = np.zeros((self.n_clusters, self.n_features))\n",
    "        for j in range(self.n_clusters):\n",
    "            num = np.sum((self.u[:, j] ** self.m)[:, np.newaxis] * self.X, axis=0)\n",
    "            denom = np.sum(self.u[:, j] ** self.m)\n",
    "            centers[j] = num / denom\n",
    "        return centers\n",
    "\n",
    "    def _update_membership(self):\n",
    "        for i in range(self.n_samples):\n",
    "            for j in range(self.n_clusters):\n",
    "                dist = np.linalg.norm(self.X[i] - self.centers[j])\n",
    "                self.u[i, j] = 1 / np.sum(\n",
    "                    (dist / np.linalg.norm(self.X[i] - self.centers[k])) ** (2 / (self.m - 1))\n",
    "                    for k in range(self.n_clusters)\n",
    "                )\n",
    "\n",
    "    def predict(self, threshold=0.5):\n",
    "        # Apply a membership threshold to assign labels\n",
    "        predictions = np.where(self.u[:, 1] > threshold, 1, 0)\n",
    "        return predictions\n",
    "\n",
    "def evaluate_model(X, y, n_runs=10, m_values=[1.5, 2.0], thresholds=[0.4, 0.5, 0.6]):\n",
    "    best_accuracy = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_report = None\n",
    "    best_m = None\n",
    "    best_threshold = None\n",
    "\n",
    "    for m in m_values:\n",
    "        for threshold in thresholds:\n",
    "            accuracy_list = []\n",
    "            precision_list = []\n",
    "            recall_list = []\n",
    "            reports = []\n",
    "\n",
    "            for _ in range(n_runs):\n",
    "                fcm = FuzzyCMeans(n_clusters=2, m=m, max_iter=150, epsilon=0.01)\n",
    "                fcm.fit(X)\n",
    "                y_pred = fcm.predict(threshold=threshold)\n",
    "\n",
    "                accuracy = accuracy_score(y, y_pred)\n",
    "                precision = precision_score(y, y_pred, zero_division=0)\n",
    "                recall = recall_score(y, y_pred, zero_division=0)\n",
    "\n",
    "                accuracy_list.append(accuracy)\n",
    "                precision_list.append(precision)\n",
    "                recall_list.append(recall)\n",
    "\n",
    "                report = classification_report(y, y_pred, target_names=['fake', 'genuine'], zero_division=0)\n",
    "                reports.append(report)\n",
    "\n",
    "            mean_accuracy = np.mean(accuracy_list)\n",
    "            mean_precision = np.mean(precision_list)\n",
    "            mean_recall = np.mean(recall_list)\n",
    "\n",
    "            if mean_accuracy > best_accuracy:\n",
    "                best_accuracy = mean_accuracy\n",
    "                best_precision = mean_precision\n",
    "                best_recall = mean_recall\n",
    "                best_report = reports[-1]\n",
    "                best_m = m\n",
    "                best_threshold = threshold\n",
    "\n",
    "    return best_accuracy, best_precision, best_recall, best_report, best_m, best_threshold\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('balanced_dataset.csv')\n",
    "    \n",
    "    # Focus on the text feature only\n",
    "    X = df['text']\n",
    "    y = df['target'].map({'genuine': 1, 'fake': 0}).values\n",
    "\n",
    "    # Convert text to TF-IDF features with n-grams\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.85, min_df=5)\n",
    "    X_tfidf = vectorizer.fit_transform(X)  # Keep sparse matrix\n",
    "\n",
    "    # Apply more aggressive TruncatedSVD to reduce dimensionality\n",
    "    svd = TruncatedSVD(n_components=25)  # Reduced further\n",
    "    X_reduced = svd.fit_transform(X_tfidf)\n",
    "\n",
    "    # Perform hyperparameter tuning for m and threshold\n",
    "    best_accuracy, best_precision, best_recall, best_report, best_m, best_threshold = evaluate_model(\n",
    "        X_reduced, y, n_runs=5, m_values=[1.5, 2.0, 2.5], thresholds=[0.3, 0.4, 0.5]\n",
    "    )\n",
    "\n",
    "    print(f'Best Mean Accuracy: {best_accuracy:.4f}')\n",
    "    print(f'Best Mean Precision: {best_precision:.4f}')\n",
    "    print(f'Best Mean Recall: {best_recall:.4f}')\n",
    "    print(f'Best m: {best_m}')\n",
    "    print(f'Best Threshold: {best_threshold}')\n",
    "\n",
    "    # Print the classification report from the best run\n",
    "    print(\"Best Classification Report:\")\n",
    "    print(best_report)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-02 22:24:38,165] A new study created in memory with name: no-name-c3c68969-bf99-4bfe-b948-f03767d5b95b\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:24:41,580] Trial 0 finished with value: 0.5 and parameters: {'m': 2.31686381382174, 'threshold': 0.32020374100490484}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:24:45,665] Trial 1 finished with value: 0.5 and parameters: {'m': 2.0336905653433783, 'threshold': 0.31883972810900374}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:24:50,211] Trial 2 finished with value: 0.5 and parameters: {'m': 2.15237711131017, 'threshold': 0.6189414596637755}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:24:53,381] Trial 3 finished with value: 0.5 and parameters: {'m': 2.2249863213595926, 'threshold': 0.43521476401540404}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:24:57,455] Trial 4 finished with value: 0.5 and parameters: {'m': 1.6953841467707478, 'threshold': 0.6653770100165004}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:01,603] Trial 5 finished with value: 0.5 and parameters: {'m': 2.1129535812527807, 'threshold': 0.5314070518757632}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:06,551] Trial 6 finished with value: 0.5 and parameters: {'m': 1.5307761634046833, 'threshold': 0.5916545770087518}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:09,813] Trial 7 finished with value: 0.5 and parameters: {'m': 2.045838671639805, 'threshold': 0.4660868806372459}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:13,382] Trial 8 finished with value: 0.5 and parameters: {'m': 2.002901574847755, 'threshold': 0.5742126981750353}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:19,561] Trial 9 finished with value: 0.5 and parameters: {'m': 1.602073271230747, 'threshold': 0.4437052909958209}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:24,442] Trial 10 finished with value: 0.5 and parameters: {'m': 2.4838632425908265, 'threshold': 0.3204486087669527}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:28,815] Trial 11 finished with value: 0.5 and parameters: {'m': 2.4032866600319887, 'threshold': 0.30116746971668723}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:34,345] Trial 12 finished with value: 0.5 and parameters: {'m': 1.8266194801941706, 'threshold': 0.39429645208615927}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:39,123] Trial 13 finished with value: 0.5 and parameters: {'m': 2.2695249801295536, 'threshold': 0.35942539560636333}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:43,506] Trial 14 finished with value: 0.5 and parameters: {'m': 1.8074264066935108, 'threshold': 0.37035287686403623}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:49,285] Trial 15 finished with value: 0.5 and parameters: {'m': 1.8783906856645332, 'threshold': 0.34176862507227557}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:53,265] Trial 16 finished with value: 0.5 and parameters: {'m': 2.3399900654233563, 'threshold': 0.4003065908409798}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:25:57,760] Trial 17 finished with value: 0.5 and parameters: {'m': 1.9279748029049548, 'threshold': 0.5081110438782228}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:01,092] Trial 18 finished with value: 0.5 and parameters: {'m': 2.216553075389581, 'threshold': 0.4050132275140543}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:05,246] Trial 19 finished with value: 0.5 and parameters: {'m': 2.497053763935515, 'threshold': 0.3347983758768001}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:08,587] Trial 20 finished with value: 0.5 and parameters: {'m': 2.34232925285314, 'threshold': 0.3003405955356571}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:12,874] Trial 21 finished with value: 0.5 and parameters: {'m': 2.1246609445192988, 'threshold': 0.6827206540207741}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:17,350] Trial 22 finished with value: 0.5 and parameters: {'m': 2.0623670443063684, 'threshold': 0.6263904389060124}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:21,394] Trial 23 finished with value: 0.5 and parameters: {'m': 2.16285124849702, 'threshold': 0.5487489931120896}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:25,498] Trial 24 finished with value: 0.5 and parameters: {'m': 1.9284756576217523, 'threshold': 0.6418484677927825}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:29,439] Trial 25 finished with value: 0.5 and parameters: {'m': 2.3163978881502194, 'threshold': 0.4734518544229819}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:32,535] Trial 26 finished with value: 0.5 and parameters: {'m': 2.2094605091887027, 'threshold': 0.5982971408019828}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:36,054] Trial 27 finished with value: 0.5 and parameters: {'m': 2.4198576387282915, 'threshold': 0.3743263382944444}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:40,243] Trial 28 finished with value: 0.5 and parameters: {'m': 2.094556806703267, 'threshold': 0.4970240072344603}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:43,685] Trial 29 finished with value: 0.5 and parameters: {'m': 2.2689611132934266, 'threshold': 0.43440919634967257}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:48,262] Trial 30 finished with value: 0.5 and parameters: {'m': 1.9708252248465112, 'threshold': 0.4193409847827462}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:53,252] Trial 31 finished with value: 0.5 and parameters: {'m': 2.194560468552769, 'threshold': 0.6972575329435932}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:26:58,508] Trial 32 finished with value: 0.5 and parameters: {'m': 2.25107661281046, 'threshold': 0.3420520374328919}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:03,346] Trial 33 finished with value: 0.5 and parameters: {'m': 2.1620852511055557, 'threshold': 0.5168024816365513}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:08,514] Trial 34 finished with value: 0.5 and parameters: {'m': 2.030611282662461, 'threshold': 0.5440725340133552}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:12,289] Trial 35 finished with value: 0.5 and parameters: {'m': 2.11324565691607, 'threshold': 0.3203015605628089}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:17,692] Trial 36 finished with value: 0.5 and parameters: {'m': 2.396746424771342, 'threshold': 0.5693279550148758}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:22,325] Trial 37 finished with value: 0.5 and parameters: {'m': 2.302345990423404, 'threshold': 0.4528281412162401}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:26,678] Trial 38 finished with value: 0.5 and parameters: {'m': 1.7598581910468305, 'threshold': 0.471215625211266}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:30,955] Trial 39 finished with value: 0.5 and parameters: {'m': 1.6563817503979084, 'threshold': 0.3785554518234758}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:35,803] Trial 40 finished with value: 0.5 and parameters: {'m': 2.006170882777085, 'threshold': 0.35694854269999565}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:41,076] Trial 41 finished with value: 0.5 and parameters: {'m': 1.568824402435708, 'threshold': 0.6694134662629037}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:46,419] Trial 42 finished with value: 0.5 and parameters: {'m': 1.6869986682595384, 'threshold': 0.6348678804529575}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:50,863] Trial 43 finished with value: 0.5 and parameters: {'m': 2.0557314476387023, 'threshold': 0.6525891062145572}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:54,055] Trial 44 finished with value: 0.5 and parameters: {'m': 2.1711299230842527, 'threshold': 0.5932243316564293}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:27:57,285] Trial 45 finished with value: 0.5 and parameters: {'m': 1.9651235070338973, 'threshold': 0.6077601288141472}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:28:00,334] Trial 46 finished with value: 0.5 and parameters: {'m': 2.3702602685822507, 'threshold': 0.32103837294305165}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:28:04,309] Trial 47 finished with value: 0.5 and parameters: {'m': 1.8633051327974453, 'threshold': 0.6602588564818157}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:28:08,413] Trial 48 finished with value: 0.5 and parameters: {'m': 2.2392820936943845, 'threshold': 0.6134094396711942}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n",
      "[I 2024-10-02 22:28:12,348] Trial 49 finished with value: 0.5 and parameters: {'m': 1.77702283998435, 'threshold': 0.5682148912651848}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_4848\\506966814.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.u[i, j] = 1 / np.sum(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: m=2.31686381382174, threshold=0.32020374100490484\n",
      "Best Accuracy: 0.5000\n",
      "Classification Report (best run):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.00      0.00      0.00     11515\n",
      "     genuine       0.50      1.00      0.67     11515\n",
      "\n",
      "    accuracy                           0.50     23030\n",
      "   macro avg       0.25      0.50      0.33     23030\n",
      "weighted avg       0.25      0.50      0.33     23030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "class FuzzyCMeans:\n",
    "    def __init__(self, n_clusters=2, m=2.0, max_iter=150, epsilon=0.01):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.m = m\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.u = np.random.rand(self.n_samples, self.n_clusters)\n",
    "        self.u = self.u / np.sum(self.u, axis=1, keepdims=True)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            self.centers = self._calculate_centers()\n",
    "            u_prev = self.u.copy()\n",
    "            self._update_membership()\n",
    "            if np.linalg.norm(self.u - u_prev) < self.epsilon:\n",
    "                break\n",
    "\n",
    "    def _calculate_centers(self):\n",
    "        centers = np.zeros((self.n_clusters, self.n_features))\n",
    "        for j in range(self.n_clusters):\n",
    "            num = np.sum((self.u[:, j] ** self.m)[:, np.newaxis] * self.X, axis=0)\n",
    "            denom = np.sum(self.u[:, j] ** self.m)\n",
    "            centers[j] = num / denom\n",
    "        return centers\n",
    "\n",
    "    def _update_membership(self):\n",
    "        for i in range(self.n_samples):\n",
    "            for j in range(self.n_clusters):\n",
    "                dist = np.linalg.norm(self.X[i] - self.centers[j])\n",
    "                self.u[i, j] = 1 / np.sum(\n",
    "                    (dist / np.linalg.norm(self.X[i] - self.centers[k])) ** (2 / (self.m - 1))\n",
    "                    for k in range(self.n_clusters)\n",
    "                )\n",
    "\n",
    "    def predict(self, threshold=0.5):\n",
    "        predictions = np.where(self.u[:, 1] > threshold, 1, 0)\n",
    "        return predictions\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    # Suggest hyperparameters\n",
    "    m = trial.suggest_float('m', 1.5, 2.5)  # Fuzziness parameter\n",
    "    threshold = trial.suggest_float('threshold', 0.3, 0.7)  # Threshold for cluster assignment\n",
    "\n",
    "    # Run Fuzzy C-Means clustering\n",
    "    fcm = FuzzyCMeans(n_clusters=2, m=m, max_iter=150, epsilon=0.01)\n",
    "    fcm.fit(X)\n",
    "    y_pred = fcm.predict(threshold=threshold)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, zero_division=0)\n",
    "    recall = recall_score(y, y_pred, zero_division=0)\n",
    "\n",
    "    # Return metric to optimize (maximize accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('balanced_dataset.csv')\n",
    "\n",
    "    # Focus on the text feature only\n",
    "    X = df['text']\n",
    "    y = df['target'].map({'genuine': 1, 'fake': 0}).values\n",
    "\n",
    "    # Convert text to TF-IDF features with n-grams\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.85, min_df=5)\n",
    "    X_tfidf = vectorizer.fit_transform(X)  # Keep sparse matrix\n",
    "\n",
    "    # Apply TruncatedSVD to reduce dimensionality\n",
    "    svd = TruncatedSVD(n_components=25)\n",
    "    X_reduced = svd.fit_transform(X_tfidf)\n",
    "\n",
    "    # Perform hyperparameter tuning using Optuna\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_reduced, y), n_trials=50)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_trial = study.best_trial\n",
    "    print(f'Best Hyperparameters: m={best_trial.params[\"m\"]}, threshold={best_trial.params[\"threshold\"]}')\n",
    "    print(f'Best Accuracy: {best_trial.value:.4f}')\n",
    "\n",
    "    # Run the model with the best hyperparameters\n",
    "    fcm = FuzzyCMeans(n_clusters=2, m=best_trial.params[\"m\"], max_iter=150, epsilon=0.01)\n",
    "    fcm.fit(X_reduced)\n",
    "    y_pred = fcm.predict(threshold=best_trial.params[\"threshold\"])\n",
    "\n",
    "    # Output the classification report\n",
    "    print(\"Classification Report (best run):\")\n",
    "    print(classification_report(y, y_pred, target_names=['fake', 'genuine'], zero_division=0))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 8.18 GiB for an array with shape (23030, 47685) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_report(y, best_labels, target_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfake\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenuine\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 80\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 57\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Apply PCA to reduce dimensionality\u001b[39;00m\n\u001b[0;32m     56\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)  \u001b[38;5;66;03m# Try reducing to 50 dimensions\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m X_tfidf_pca \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Split data for evaluation (using the whole dataset here, but you can use train-test split)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Optuna hyperparameter optimization\u001b[39;00m\n\u001b[0;32m     62\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:454\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:483\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_array_api_compliant:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for Array API inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    481\u001b[0m     )\n\u001b[1;32m--> 483\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1060\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[1;32m-> 1060\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;66;03m# always make a copy for non-numpy arrays\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m     array \u001b[38;5;241m=\u001b[39m _asarray_with_order(\n\u001b[0;32m   1066\u001b[0m         array, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[0;32m   1067\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:519\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 519\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    521\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 8.18 GiB for an array with shape (23030, 47685) and data type float64"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from fcmeans import FCM\n",
    "import optuna\n",
    "\n",
    "# Function to apply FCM and return accuracy, precision, recall\n",
    "def evaluate_fcm(X, y, m, threshold, n_clusters=2, n_init=10):\n",
    "    best_accuracy = 0\n",
    "    best_fcm = None\n",
    "    best_labels = None\n",
    "\n",
    "    for _ in range(n_init):  # Repeat with different initializations\n",
    "        fcm = FCM(n_clusters=n_clusters, m=m)\n",
    "        fcm.fit(X)\n",
    "        membership = fcm.u.T\n",
    "        \n",
    "        # Post-process membership values with rule-based approach\n",
    "        predicted_labels = np.argmax(membership, axis=0)\n",
    "        memberships_diff = np.abs(membership[0] - membership[1])\n",
    "        predicted_labels = np.where(memberships_diff > threshold, predicted_labels, 1 - predicted_labels)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        accuracy = accuracy_score(y, predicted_labels)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_fcm = fcm\n",
    "            best_labels = predicted_labels\n",
    "\n",
    "    return best_accuracy, best_fcm, best_labels\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters for fuzziness and threshold\n",
    "    m = trial.suggest_float('m', 1.5, 3.0)\n",
    "    threshold = trial.suggest_float('threshold', 0.2, 0.8)\n",
    "    \n",
    "    # Evaluate FCM with sampled hyperparameters\n",
    "    accuracy, _, _ = evaluate_fcm(X_tfidf_pca, y, m, threshold)\n",
    "    return 1.0 - accuracy  # Minimize 1 - accuracy\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load your dataset (adjust path as necessary)\n",
    "    df = pd.read_csv('balanced_dataset.csv')\n",
    "    X = df['text'].values\n",
    "    y = np.where(df['target'] == 'fake', 0, 1)  # Convert labels to binary\n",
    "\n",
    "    # Convert text to TF-IDF features with n-grams\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.9, min_df=5)\n",
    "    X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "    # Apply PCA to reduce dimensionality\n",
    "    pca = PCA(n_components=50)  # Try reducing to 50 dimensions\n",
    "    X_tfidf_pca = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "    # Split data for evaluation (using the whole dataset here, but you can use train-test split)\n",
    "    \n",
    "    # Optuna hyperparameter optimization\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50)  # Try with 50 trials\n",
    "    \n",
    "    # Best hyperparameters\n",
    "    best_m = study.best_params['m']\n",
    "    best_threshold = study.best_params['threshold']\n",
    "    print(f\"Best Hyperparameters: m={best_m}, threshold={best_threshold}\")\n",
    "    \n",
    "    # Evaluate FCM with best hyperparameters\n",
    "    best_accuracy, best_fcm, best_labels = evaluate_fcm(X_tfidf_pca, y, best_m, best_threshold)\n",
    "    \n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y, best_labels, target_names=['fake', 'genuine']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n",
      "[[ 1.41249225e-01 -6.48040323e-03  1.32928309e-03 -1.02808579e-03\n",
      "   1.48203406e-03 -2.32888986e-03 -2.47557687e-03 -7.24679766e-04\n",
      "   1.62497116e-03 -5.67599033e-04 -1.60320204e-03  9.46230889e-04\n",
      "   8.66565799e-04  1.28122511e-03  3.06109327e-04  9.65973806e-04\n",
      "  -2.05333166e-04  9.20409479e-04  1.47464697e-03 -1.03631372e-03\n",
      "  -3.28369969e-04 -9.65071675e-05  5.90302539e-04  1.65437258e-04\n",
      "   2.88683909e-04  8.18172998e-05  6.80605209e-04 -1.00075414e-03\n",
      "  -4.20520627e-04 -5.80258822e-04  9.97854158e-04  1.09460433e-04\n",
      "   9.40761964e-04 -2.30986410e-04  2.97478821e-04  6.68998484e-04\n",
      "  -7.61014832e-05 -4.13496168e-04  7.99675116e-04 -6.07181054e-04\n",
      "  -6.70839258e-05 -8.94304603e-05  5.73961689e-04  5.95883050e-04\n",
      "  -8.41201284e-04  1.25878357e-04  1.30785632e-04  2.11379467e-04\n",
      "   5.23140604e-04 -3.99273065e-04  5.24185792e-04  8.91776102e-04\n",
      "  -1.81064059e-04  1.34546930e-04  1.63387618e-05  3.41091548e-04\n",
      "  -6.22517192e-04 -3.10770338e-04 -2.93766798e-04 -1.86417298e-04\n",
      "   4.80690287e-04  4.59914602e-04  3.38453822e-04  3.57527915e-04\n",
      "  -1.10619430e-04  2.76484363e-04 -2.48631860e-05 -2.78186348e-04\n",
      "   2.99586884e-04  2.14589259e-04 -1.11052385e-04  1.01538242e-04\n",
      "   2.02558184e-04 -7.72152621e-07  7.80332846e-04  1.32054997e-05\n",
      "   1.28742291e-04  2.45166919e-04 -4.63198778e-04  8.01868741e-06\n",
      "   2.34798799e-04 -8.28884935e-05 -3.47039120e-04 -6.09671079e-05\n",
      "   2.78296348e-04 -5.60465791e-06 -5.14852020e-04  3.29145184e-04\n",
      "  -6.25843733e-05  3.78503779e-04 -5.46817870e-04 -1.81240210e-05\n",
      "   5.55813200e-05  7.14168389e-05  3.77104846e-05 -2.84534210e-04\n",
      "  -1.63952635e-04  4.13142507e-05  8.03284770e-05  1.11286606e-04]\n",
      " [ 1.41253009e-01 -6.47789476e-03  1.32869961e-03 -1.02776698e-03\n",
      "   1.48157502e-03 -2.32836965e-03 -2.47547251e-03 -7.24116447e-04\n",
      "   1.62483716e-03 -5.67374027e-04 -1.60258814e-03  9.46091130e-04\n",
      "   8.66291199e-04  1.28111688e-03  3.06025404e-04  9.65866687e-04\n",
      "  -2.05339390e-04  9.20170338e-04  1.47441900e-03 -1.03609525e-03\n",
      "  -3.28268318e-04 -9.65572173e-05  5.90356797e-04  1.65369839e-04\n",
      "   2.88556093e-04  8.17796673e-05  6.80523676e-04 -1.00061453e-03\n",
      "  -4.20429656e-04 -5.80149755e-04  9.97786071e-04  1.09466015e-04\n",
      "   9.40595139e-04 -2.30856344e-04  2.97353507e-04  6.68948109e-04\n",
      "  -7.60589502e-05 -4.13368546e-04  7.99508126e-04 -6.07021491e-04\n",
      "  -6.70128825e-05 -8.93638859e-05  5.73890691e-04  5.95827327e-04\n",
      "  -8.41161559e-04  1.25865625e-04  1.30763226e-04  2.11343462e-04\n",
      "   5.23091496e-04 -3.99235398e-04  5.24198803e-04  8.91586993e-04\n",
      "  -1.81004170e-04  1.34500426e-04  1.63065854e-05  3.41055038e-04\n",
      "  -6.22411075e-04 -3.10764833e-04 -2.93683842e-04 -1.86329568e-04\n",
      "   4.80748630e-04  4.59901992e-04  3.38410068e-04  3.57496438e-04\n",
      "  -1.10561814e-04  2.76407641e-04 -2.48503761e-05 -2.78102086e-04\n",
      "   2.99488011e-04  2.14623962e-04 -1.10984173e-04  1.01477225e-04\n",
      "   2.02494207e-04 -7.84240449e-07  7.80118232e-04  1.32340558e-05\n",
      "   1.28722493e-04  2.45135574e-04 -4.63122233e-04  7.98788266e-06\n",
      "   2.34748727e-04 -8.28350584e-05 -3.46954799e-04 -6.09866351e-05\n",
      "   2.78159947e-04 -5.53896236e-06 -5.14752611e-04  3.29058282e-04\n",
      "  -6.25433255e-05  3.78423507e-04 -5.46702590e-04 -1.81643355e-05\n",
      "   5.55740189e-05  7.14058034e-05  3.77704203e-05 -2.84423674e-04\n",
      "  -1.63888501e-04  4.12894407e-05  8.03185938e-05  1.11292589e-04]]\n",
      "Accuracy: 0.4756838905775076\n",
      "Precision: 0.4755860395458607\n",
      "Recall: 0.4756838905775076\n",
      "Confusion Matrix:\n",
      "[[5842 5673]\n",
      " [6402 5113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.51      0.49     11515\n",
      "           1       0.47      0.44      0.46     11515\n",
      "\n",
      "    accuracy                           0.48     23030\n",
      "   macro avg       0.48      0.48      0.48     23030\n",
      "weighted avg       0.48      0.48      0.48     23030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract relevant columns\n",
    "    df = df[['text', 'target']]\n",
    "    \n",
    "    # Convert text data to numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(df['text'])\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    svd = TruncatedSVD(n_components=100)  # Reduce to 100 dimensions\n",
    "    X = svd.fit_transform(X)\n",
    "    \n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df['target'])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def fuzzy_c_means(X, k, m=2.0, max_iter=100, epsilon=0.01):\n",
    "    n_samples, n_features = X.shape\n",
    "    u = np.random.rand(n_samples, k)\n",
    "    u = u / np.sum(u, axis=1, keepdims=True)  # Normalize membership matrix\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        u_prev = u.copy()\n",
    "        cluster_centers = np.dot(u.T ** m, X) / np.sum(u.T ** m, axis=1, keepdims=True)\n",
    "\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - cluster_centers, axis=2)\n",
    "        u = 1.0 / np.sum((distances[:, :, np.newaxis] / distances[:, np.newaxis]) ** (2 / (m - 1)), axis=2)\n",
    "\n",
    "        # Check convergence\n",
    "        if np.linalg.norm(u - u_prev) < epsilon:\n",
    "            break\n",
    "\n",
    "    labels = np.argmax(u, axis=1)\n",
    "    return labels, cluster_centers\n",
    "\n",
    "def evaluate_clustering(labels, y):\n",
    "    accuracy = accuracy_score(y, labels)\n",
    "    precision = precision_score(y, labels, average='weighted')\n",
    "    recall = recall_score(y, labels, average='weighted')\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "# File path to the dataset\n",
    "file_path = 'balanced_dataset.csv'\n",
    "\n",
    "# Preprocess data\n",
    "X, y = preprocess_data(file_path)\n",
    "\n",
    "# Perform Fuzzy C-means clustering\n",
    "labels, centers = fuzzy_c_means(X, k=2, m=2.0, max_iter=100, epsilon=0.01)\n",
    "\n",
    "# Print cluster centers\n",
    "print(\"Cluster Centers:\")\n",
    "print(centers)\n",
    "\n",
    "# Evaluate clustering\n",
    "accuracy, precision, recall = evaluate_clustering(labels, y)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y, labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(y, labels)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUZZY 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n",
      "[[ 0.00190953  0.00017815  0.0005267  ...  0.00056102 -0.06908985\n",
      "  -0.05793134]\n",
      " [ 0.00190953  0.00017815  0.0005267  ...  0.00056102 -0.06908985\n",
      "  -0.05793134]]\n",
      "Accuracy: 0.4742075553625706\n",
      "Precision: 0.473218383556886\n",
      "Recall: 0.4742075553625706\n",
      "Confusion Matrix:\n",
      "[[6567 4948]\n",
      " [7161 4354]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.57      0.52     11515\n",
      "           1       0.47      0.38      0.42     11515\n",
      "\n",
      "    accuracy                           0.47     23030\n",
      "   macro avg       0.47      0.47      0.47     23030\n",
      "weighted avg       0.47      0.47      0.47     23030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract relevant columns\n",
    "    df = df[['text', 'useful', 'funny', 'target']]\n",
    "    \n",
    "    # Convert text data to numerical features using sparse TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # Limit features to reduce dimensionality\n",
    "    text_features = vectorizer.fit_transform(df['text'])\n",
    "    \n",
    "    # Extract and scale numeric features\n",
    "    numeric_features = df[['useful', 'funny']].fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    numeric_features_scaled = scaler.fit_transform(numeric_features)\n",
    "    \n",
    "    # Dimensionality reduction for numeric features using PCA\n",
    "    pca = PCA(n_components=2)  # Limit to 2 components\n",
    "    numeric_features_reduced = pca.fit_transform(numeric_features_scaled)\n",
    "    \n",
    "    # Combine sparse text features with reduced numeric features\n",
    "    X = csr_matrix(np.hstack([text_features.toarray(), numeric_features_reduced]))  # Combine sparse and dense arrays\n",
    "    \n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df['target'])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def fuzzy_c_means(X, k, m=2.0, max_iter=100, epsilon=0.01, batch_size=5000):\n",
    "    n_samples, n_features = X.shape\n",
    "    u = np.random.rand(n_samples, k)\n",
    "    u = u / np.sum(u, axis=1, keepdims=True)  # Normalize membership matrix\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        u_prev = u.copy()\n",
    "        \n",
    "        # Batch processing\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            X_batch = X[start:end]\n",
    "            u_batch = u[start:end]\n",
    "            \n",
    "            # Compute cluster centers\n",
    "            u_m = u_batch ** m\n",
    "            cluster_centers = np.dot(u_m.T, X_batch.toarray()) / np.sum(u_m, axis=0, keepdims=True).T\n",
    "            \n",
    "            # Compute distances\n",
    "            distances = np.linalg.norm(X_batch.toarray()[:, np.newaxis] - cluster_centers, axis=2)\n",
    "            u[start:end] = 1.0 / np.sum((distances[:, :, np.newaxis] / distances[:, np.newaxis]) ** (2 / (m - 1)), axis=2)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.linalg.norm(u - u_prev) < epsilon:\n",
    "            break\n",
    "\n",
    "    labels = np.argmax(u, axis=1)\n",
    "    return labels, cluster_centers\n",
    "\n",
    "def evaluate_clustering(labels, y):\n",
    "    accuracy = accuracy_score(y, labels)\n",
    "    precision = precision_score(y, labels, average='weighted')\n",
    "    recall = recall_score(y, labels, average='weighted')\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "# File path to the dataset\n",
    "file_path = 'balanced_dataset.csv'\n",
    "\n",
    "# Preprocess data\n",
    "X, y = preprocess_data(file_path)\n",
    "\n",
    "# Perform Fuzzy C-means clustering\n",
    "labels, centers = fuzzy_c_means(X, k=2, m=2.0, max_iter=100, epsilon=0.01)\n",
    "\n",
    "# Print cluster centers\n",
    "print(\"Cluster Centers:\")\n",
    "print(centers)\n",
    "\n",
    "# Evaluate clustering\n",
    "accuracy, precision, recall = evaluate_clustering(labels, y)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y, labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(y, labels)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-fuzzy in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.2)\n",
      "Collecting scikit-fuzzy\n",
      "  Downloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl (920 kB)\n",
      "   ---------------------------------------- 0.0/920.8 kB ? eta -:--:--\n",
      "    -------------------------------------- 20.5/920.8 kB 330.3 kB/s eta 0:00:03\n",
      "   - ------------------------------------- 30.7/920.8 kB 217.9 kB/s eta 0:00:05\n",
      "   - ------------------------------------- 41.0/920.8 kB 326.8 kB/s eta 0:00:03\n",
      "   - ------------------------------------- 41.0/920.8 kB 326.8 kB/s eta 0:00:03\n",
      "   ------ ------------------------------- 153.6/920.8 kB 612.6 kB/s eta 0:00:02\n",
      "   ----------- -------------------------- 286.7/920.8 kB 983.9 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 870.4/920.8 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 920.8/920.8 kB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-fuzzy\n",
      "  Attempting uninstall: scikit-fuzzy\n",
      "    Found existing installation: scikit-fuzzy 0.4.2\n",
      "    Uninstalling scikit-fuzzy-0.4.2:\n",
      "      Successfully uninstalled scikit-fuzzy-0.4.2\n",
      "Successfully installed scikit-fuzzy-0.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-fuzzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n",
      "[[ 3.70094091e-02  3.67136743e-02  1.62563398e-02  2.21671941e-03\n",
      "  -1.36338976e-02 -9.15482290e-03  5.25985093e-03 -8.02427284e-03\n",
      "   2.15671202e-04  2.09489596e-03 -4.31139607e-03 -2.89895003e-03\n",
      "   4.32322056e-03 -1.61995379e-03 -8.74647473e-03  5.72593010e-04\n",
      "   6.50987273e-04 -5.69423895e-04  1.87966422e-03 -3.52848393e-03\n",
      "  -1.52310877e-03  8.15577359e-05 -1.53271485e-03  2.20915115e-03\n",
      "  -8.71019151e-06  1.57115749e-03  1.20074316e-03  2.06878224e-04\n",
      "   1.21566023e-03  3.18093838e-03 -1.28023245e-03 -3.21380705e-04\n",
      "   1.18245054e-04 -2.86533548e-04  7.37629419e-04  8.13854227e-04\n",
      "   5.14093428e-04 -2.60189124e-03 -1.16392241e-04 -1.00190302e-03\n",
      "   1.68803122e-03  1.65985472e-04 -2.10803822e-03  2.30748885e-04\n",
      "  -3.55984268e-04 -2.56217808e-03 -1.39399717e-04  1.58779635e-03\n",
      "  -1.82401855e-03 -7.16958504e-04  1.99229546e+00 -4.88845694e-01]\n",
      " [-6.18166787e-03 -5.88566149e-03 -2.81660517e-03 -6.01311204e-04\n",
      "   1.93759209e-03  1.30330587e-03 -8.87469132e-04  1.10652674e-03\n",
      "   1.34678532e-04 -4.09055830e-04  5.85001569e-04  2.00957055e-04\n",
      "  -5.47324156e-04  5.09253835e-05  1.48037803e-03 -6.58842976e-05\n",
      "  -1.15518163e-04  1.01825218e-04 -3.07163313e-04  4.98878593e-04\n",
      "   2.16839923e-04 -1.45309481e-04  2.18532045e-04 -3.47325050e-04\n",
      "   6.94030675e-05 -2.69323457e-04 -2.07614938e-04 -3.85774674e-05\n",
      "  -1.61970298e-04 -5.27180994e-04  8.05838856e-05  9.41279353e-05\n",
      "   3.02778823e-05 -2.34636569e-05 -1.93221453e-04 -1.51702936e-04\n",
      "  -4.02447358e-05  3.83083933e-04  5.00085406e-05  5.10843672e-05\n",
      "  -2.81076223e-04 -2.54588083e-05  2.51647633e-04 -9.10882802e-05\n",
      "   5.93054810e-05  3.60454391e-04  2.07273749e-05 -1.73656776e-04\n",
      "   2.44754488e-04  6.90783652e-05 -2.92197638e-01  6.00549539e-02]]\n",
      "Accuracy: 0.5163699522362136\n",
      "Precision: 0.5091084803092535\n",
      "Recall: 0.9149804602692141\n",
      "F1 Score: 0.6542067680844458\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from skfuzzy.cluster import cmeans\n",
    "from scipy.sparse import csr_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Select relevant features\n",
    "    df = df[['text', 'useful', 'funny', 'target']]\n",
    "    \n",
    "    # Split data into text and numeric features\n",
    "    text_data = df['text']\n",
    "    numeric_features = df[['useful', 'funny']]\n",
    "\n",
    "    # Text vectorization using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=10000)\n",
    "    text_features = vectorizer.fit_transform(text_data)\n",
    "    \n",
    "    # Dimensionality reduction for text features using PCA\n",
    "    pca = PCA(n_components=50)\n",
    "    text_features_reduced = pca.fit_transform(text_features.toarray())\n",
    "\n",
    "    # Standardize numeric features\n",
    "    scaler = StandardScaler()\n",
    "    numeric_features_scaled = scaler.fit_transform(numeric_features)\n",
    "    \n",
    "    # Apply PCA on numeric features\n",
    "    pca_numeric = PCA(n_components=2)\n",
    "    numeric_features_reduced = pca_numeric.fit_transform(numeric_features_scaled)\n",
    "    \n",
    "    # Combine TF-IDF features with numeric features\n",
    "    combined_features = np.hstack([text_features_reduced, numeric_features_reduced])\n",
    "    \n",
    "    # Get the target labels (convert 'fake'/'genuine' to 0/1)\n",
    "    y = df['target'].apply(lambda x: 1 if x == 'genuine' else 0).values\n",
    "    \n",
    "    # Handle class imbalance\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(combined_features, y)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Fuzzy C-means clustering\n",
    "def fuzzy_c_means(X, k=2, m=2.0, max_iter=300, epsilon=1e-6):\n",
    "    # Apply fuzzy c-means clustering\n",
    "    cntr, u, u0, d, jm, p, fpc = cmeans(X.T, c=k, m=m, error=epsilon, maxiter=max_iter, init=None)\n",
    "    \n",
    "    # Predict cluster membership\n",
    "    labels = np.argmax(u, axis=0)\n",
    "    \n",
    "    return labels, cntr\n",
    "\n",
    "# Evaluation metrics\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your CSV file\n",
    "    file_path = 'balanced_dataset.csv'\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X, y = preprocess_data(file_path)\n",
    "    \n",
    "    # Perform Fuzzy C-means clustering with adjusted parameters\n",
    "    labels, centers = fuzzy_c_means(X, k=2, m=2.0, max_iter=300, epsilon=1e-6)\n",
    "    \n",
    "    # Print cluster centers\n",
    "    print(\"Cluster Centers:\")\n",
    "    print(centers)\n",
    "    \n",
    "    # Evaluate the clustering model\n",
    "    evaluate_model(y, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Silhouette Score: 0.41281517565943665\n",
      "Training Davies-Bouldin Index: 1.0537244105605363\n",
      "Test Silhouette Score: 0.40909542112034936\n",
      "Test Davies-Bouldin Index: 1.0778565120614865\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FuzzyCMeans:\n",
    "    def __init__(self, n_clusters=2, m=2.0, max_iter=100, epsilon=0.01):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.m = m\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.u = np.random.rand(self.n_samples, self.n_clusters)\n",
    "        self.u = self.u / np.sum(self.u, axis=1, keepdims=True)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            self.centers = self._calculate_centers()\n",
    "            u_prev = self.u.copy()\n",
    "            self._update_membership()\n",
    "            if np.linalg.norm(self.u - u_prev) < self.epsilon:\n",
    "                break\n",
    "\n",
    "    def _calculate_centers(self):\n",
    "        centers = np.zeros((self.n_clusters, self.n_features))\n",
    "        for j in range(self.n_clusters):\n",
    "            num = np.sum((self.u[:, j] ** self.m)[:, np.newaxis] * self.X, axis=0)\n",
    "            denom = np.sum(self.u[:, j] ** self.m)\n",
    "            centers[j] = num / denom\n",
    "        return centers\n",
    "\n",
    "    def _update_membership(self):\n",
    "        for i in range(self.n_samples):\n",
    "            for j in range(self.n_clusters):\n",
    "                dist = np.linalg.norm(self.X[i] - self.centers[j])\n",
    "                self.u[i, j] = 1 / sum(\n",
    "                    (dist / np.linalg.norm(self.X[i] - self.centers[k])) ** (2 / (self.m - 1))\n",
    "                    for k in range(self.n_clusters)\n",
    "                )\n",
    "\n",
    "    def predict(self):\n",
    "        return np.argmax(self.u, axis=1)\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('balanced_dataset.csv')\n",
    "    features = ['stars', 'useful', 'funny']  # Use relevant numerical features\n",
    "    X = df[features].values\n",
    "    y = df['target'].map({'genuine': 1, 'fake': 0}).values\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fuzzy C-means clustering\n",
    "    fcm = FuzzyCMeans(n_clusters=2, m=2.0, max_iter=100, epsilon=0.01)\n",
    "    fcm.fit(X_train)\n",
    "    y_pred_train = fcm.predict()\n",
    "\n",
    "    # Clustering evaluation metrics\n",
    "    silhouette_avg = silhouette_score(X_train, y_pred_train)\n",
    "    davies_bouldin_avg = davies_bouldin_score(X_train, y_pred_train)\n",
    "\n",
    "    print(f'Training Silhouette Score: {silhouette_avg}')\n",
    "    print(f'Training Davies-Bouldin Index: {davies_bouldin_avg}')\n",
    "\n",
    "    # Predict on the test set\n",
    "    fcm.fit(X_test)\n",
    "    y_pred_test = fcm.predict()\n",
    "\n",
    "    # Clustering evaluation metrics for test data\n",
    "    silhouette_avg_test = silhouette_score(X_test, y_pred_test)\n",
    "    davies_bouldin_avg_test = davies_bouldin_score(X_test, y_pred_test)\n",
    "\n",
    "    print(f'Test Silhouette Score: {silhouette_avg_test}')\n",
    "    print(f'Test Davies-Bouldin Index: {davies_bouldin_avg_test}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7260095527572731\n",
      "Precision: 0.7361405634656165\n",
      "Recall: 0.7039397450753186\n",
      "F1 Score: 0.7196801421590404\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('balanced_dataset.csv')\n",
    "\n",
    "# Text preprocessing: Convert the text to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(df['text']).toarray()  # Convert text to numerical form\n",
    "y = df['target'].map({'fake': 0, 'genuine': 1}).values  # Map labels to 0 and 1\n",
    "\n",
    "# Dimensionality reduction with PCA (try reducing features to 50)\n",
    "pca = PCA(n_components=50)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fuzzy C-Means clustering function\n",
    "def fuzzy_c_means(X, n_clusters, m=2.0, error=0.005, maxiter=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialize fuzzy partition matrix randomly\n",
    "    u = np.random.rand(n_samples, n_clusters)\n",
    "    u = u / np.sum(u, axis=1, keepdims=True)\n",
    "\n",
    "    # Run the clustering algorithm\n",
    "    for i in range(maxiter):\n",
    "        u_old = u.copy()\n",
    "\n",
    "        # Calculate cluster centers\n",
    "        centers = np.dot(u.T**m, X) / np.sum(u.T**m, axis=1, keepdims=True)\n",
    "\n",
    "        # Update membership matrix\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centers, axis=2)\n",
    "        distances = np.fmax(distances, np.finfo(np.float64).eps)\n",
    "\n",
    "        u = 1 / distances**(2/(m-1))\n",
    "        u = u / np.sum(u, axis=1, keepdims=True)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(u - u_old) < error:\n",
    "            break\n",
    "\n",
    "    return centers, u\n",
    "\n",
    "# Set number of clusters\n",
    "n_clusters = 2\n",
    "\n",
    "# Apply Fuzzy C-Means clustering on training data\n",
    "centers, u_train = fuzzy_c_means(X_train, n_clusters)\n",
    "\n",
    "# Use the membership values (soft assignments) from FCM as features\n",
    "# Apply Fuzzy C-Means on test data\n",
    "_, u_test = fuzzy_c_means(X_test, n_clusters)\n",
    "\n",
    "# Concatenate FCM membership features with original features\n",
    "X_train_augmented = np.hstack([X_train, u_train])\n",
    "X_test_augmented = np.hstack([X_test, u_test])\n",
    "\n",
    "# Train a Logistic Regression model using the augmented feature space\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_augmented, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test_augmented)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Silhouette Score: 0.4128\n",
      "Training Davies-Bouldin Index: 1.0537\n",
      "Training Accuracy: 0.9375\n",
      "Training Precision: 0.9911\n",
      "Training Recall: 0.8828\n",
      "Training F1 Score: 0.9338\n",
      "Test Silhouette Score: 0.4081\n",
      "Test Davies-Bouldin Index: 1.0762\n",
      "Test Accuracy: 0.9368\n",
      "Test Precision: 0.9927\n",
      "Test Recall: 0.8807\n",
      "Test F1 Score: 0.9334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FuzzyCMeans:\n",
    "    def __init__(self, n_clusters=2, m=2.0, max_iter=100, epsilon=0.01):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.m = m\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.u = np.random.rand(self.n_samples, self.n_clusters)\n",
    "        self.u = self.u / np.sum(self.u, axis=1, keepdims=True)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            self.centers = self._calculate_centers()\n",
    "            u_prev = self.u.copy()\n",
    "            self._update_membership()\n",
    "            if np.linalg.norm(self.u - u_prev) < self.epsilon:\n",
    "                break\n",
    "\n",
    "    def _calculate_centers(self):\n",
    "        centers = np.zeros((self.n_clusters, self.n_features))\n",
    "        for j in range(self.n_clusters):\n",
    "            num = np.sum((self.u[:, j] ** self.m)[:, np.newaxis] * self.X, axis=0)\n",
    "            denom = np.sum(self.u[:, j] ** self.m)\n",
    "            centers[j] = num / denom\n",
    "        return centers\n",
    "\n",
    "    def _update_membership(self):\n",
    "        for i in range(self.n_samples):\n",
    "            for j in range(self.n_clusters):\n",
    "                dist = np.linalg.norm(self.X[i] - self.centers[j])\n",
    "                self.u[i, j] = 1 / sum(\n",
    "                    (dist / np.linalg.norm(self.X[i] - self.centers[k])) ** (2 / (self.m - 1))\n",
    "                    for k in range(self.n_clusters)\n",
    "                )\n",
    "\n",
    "    def predict(self, X):\n",
    "        u = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(self.n_clusters):\n",
    "                dist = np.linalg.norm(X[i] - self.centers[j])\n",
    "                u[i, j] = 1 / sum(\n",
    "                    (dist / np.linalg.norm(X[i] - self.centers[k])) ** (2 / (self.m - 1))\n",
    "                    for k in range(self.n_clusters)\n",
    "                )\n",
    "        return np.argmax(u, axis=1)\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('processed_balanced_dataset.csv')\n",
    "    features = ['stars', 'useful', 'funny']  # Use relevant numerical features\n",
    "    X = df[features].values\n",
    "    y = df['target'].map({'genuine': 1, 'fake': 0}).values\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fuzzy C-means clustering\n",
    "    fcm = FuzzyCMeans(n_clusters=2, m=2.0, max_iter=100, epsilon=0.01)\n",
    "    fcm.fit(X_train)\n",
    "\n",
    "    # Predict clusters on training data\n",
    "    y_pred_train = fcm.predict(X_train)\n",
    "\n",
    "    # Clustering evaluation metrics for training data\n",
    "    silhouette_avg_train = silhouette_score(X_train, y_pred_train)\n",
    "    davies_bouldin_avg_train = davies_bouldin_score(X_train, y_pred_train)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    recall_train = recall_score(y_train, y_pred_train)\n",
    "    f1_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "    print(f'Training Silhouette Score: {silhouette_avg_train:.4f}')\n",
    "    print(f'Training Davies-Bouldin Index: {davies_bouldin_avg_train:.4f}')\n",
    "    print(f'Training Accuracy: {accuracy_train:.4f}')\n",
    "    print(f'Training Precision: {precision_train:.4f}')\n",
    "    print(f'Training Recall: {recall_train:.4f}')\n",
    "    print(f'Training F1 Score: {f1_train:.4f}')\n",
    "\n",
    "    # Predict clusters on test data\n",
    "    y_pred_test = fcm.predict(X_test)\n",
    "\n",
    "    # Clustering evaluation metrics for test data\n",
    "    silhouette_avg_test = silhouette_score(X_test, y_pred_test)\n",
    "    davies_bouldin_avg_test = davies_bouldin_score(X_test, y_pred_test)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "    f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f'Test Silhouette Score: {silhouette_avg_test:.4f}')\n",
    "    print(f'Test Davies-Bouldin Index: {davies_bouldin_avg_test:.4f}')\n",
    "    print(f'Test Accuracy: {accuracy_test:.4f}')\n",
    "    print(f'Test Precision: {precision_test:.4f}')\n",
    "    print(f'Test Recall: {recall_test:.4f}')\n",
    "    print(f'Test F1 Score: {f1_test:.4f}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.5722\n",
      "Training Precision: 0.5689\n",
      "Training Recall: 0.5925\n",
      "Training F1 Score: 0.5805\n",
      "Test Accuracy: 0.5721\n",
      "Test Precision: 0.5723\n",
      "Test Recall: 0.5864\n",
      "Test F1 Score: 0.5793\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class FuzzyCMeans:\n",
    "    def __init__(self, n_clusters=2, m=2.0, max_iter=100, epsilon=0.01):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.m = m\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.u = np.random.rand(self.n_samples, self.n_clusters)\n",
    "        self.u = self.u / np.sum(self.u, axis=1, keepdims=True)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            self.centers = self._calculate_centers()\n",
    "            u_prev = self.u.copy()\n",
    "            self._update_membership()\n",
    "            if np.linalg.norm(self.u - u_prev) < self.epsilon:\n",
    "                break\n",
    "\n",
    "    def _calculate_centers(self):\n",
    "        centers = np.zeros((self.n_clusters, self.n_features))\n",
    "        for j in range(self.n_clusters):\n",
    "            num = np.sum((self.u[:, j] ** self.m)[:, np.newaxis] * self.X, axis=0)\n",
    "            denom = np.sum(self.u[:, j] ** self.m)\n",
    "            centers[j] = num / denom\n",
    "        return centers\n",
    "\n",
    "    def _update_membership(self):\n",
    "        for i in range(self.n_samples):\n",
    "            for j in range(self.n_clusters):\n",
    "                dist = np.linalg.norm(self.X[i] - self.centers[j])\n",
    "                self.u[i, j] = 1 / sum(\n",
    "                    (dist / np.linalg.norm(self.X[i] - self.centers[k])) ** (2 / (self.m - 1))\n",
    "                    for k in range(self.n_clusters)\n",
    "                )\n",
    "\n",
    "    def predict(self, X):\n",
    "        u = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(self.n_clusters):\n",
    "                dist = np.linalg.norm(X[i] - self.centers[j])\n",
    "                u[i, j] = 1 / sum(\n",
    "                    (dist / np.linalg.norm(X[i] - self.centers[k])) ** (2 / (self.m - 1))\n",
    "                    for k in range(self.n_clusters)\n",
    "                )\n",
    "        return np.argmax(u, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        u = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(self.n_clusters):\n",
    "                dist = np.linalg.norm(X[i] - self.centers[j])\n",
    "                u[i, j] = 1 / sum(\n",
    "                    (dist / np.linalg.norm(X[i] - self.centers[k])) ** (2 / (self.m - 1))\n",
    "                    for k in range(self.n_clusters)\n",
    "                )\n",
    "        return u\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv('balanced_dataset.csv')\n",
    "\n",
    "    # Process the text attribute\n",
    "    text = df['text']\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "    X = tfidf_vectorizer.fit_transform(text).toarray()\n",
    "    y = df['target'].map({'genuine': 1, 'fake': 0}).values\n",
    "\n",
    "    # Encode target labels if necessary\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fuzzy C-means clustering\n",
    "    fcm = FuzzyCMeans(n_clusters=2, m=2.0, max_iter=100, epsilon=0.01)\n",
    "    fcm.fit(X_train)\n",
    "\n",
    "    # Predict cluster membership probabilities\n",
    "    u_train = fcm.predict_proba(X_train)\n",
    "    u_test = fcm.predict_proba(X_test)\n",
    "\n",
    "    # Assign cluster labels based on majority class in each cluster\n",
    "    cluster_labels = np.zeros(fcm.n_clusters)\n",
    "    for j in range(fcm.n_clusters):\n",
    "        cluster_labels[j] = np.bincount(y_train[np.argmax(u_train, axis=1) == j]).argmax()\n",
    "\n",
    "    # Predict class labels for training and test data\n",
    "    y_pred_train = np.array([cluster_labels[np.argmax(u)] for u in u_train])\n",
    "    y_pred_test = np.array([cluster_labels[np.argmax(u)] for u in u_test])\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f'Training Accuracy: {accuracy_score(y_train, y_pred_train):.4f}')\n",
    "    print(f'Training Precision: {precision_score(y_train, y_pred_train):.4f}')\n",
    "    print(f'Training Recall: {recall_score(y_train, y_pred_train):.4f}')\n",
    "    print(f'Training F1 Score: {f1_score(y_train, y_pred_train):.4f}')\n",
    "    \n",
    "    print(f'Test Accuracy: {accuracy_score(y_test, y_pred_test):.4f}')\n",
    "    print(f'Test Precision: {precision_score(y_test, y_pred_test):.4f}')\n",
    "    print(f'Test Recall: {recall_score(y_test, y_pred_test):.4f}')\n",
    "    print(f'Test F1 Score: {f1_score(y_test, y_pred_test):.4f}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.99      0.98      0.98      2166\n",
      "     genuine       0.98      0.99      0.98      2440\n",
      "\n",
      "    accuracy                           0.98      4606\n",
      "   macro avg       0.98      0.98      0.98      4606\n",
      "weighted avg       0.98      0.98      0.98      4606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fcmeans import FCM\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Step 2: Load datasets\n",
    "unlabelled_file = 'dataset_without_target.csv'  # Dataset without target labels\n",
    "labelled_file = 'processed_balanced_dataset.csv'  # Original dataset with target labels\n",
    "\n",
    "unlabelled_df = pd.read_csv(unlabelled_file)\n",
    "labelled_df = pd.read_csv(labelled_file)\n",
    "\n",
    "# Step 3: Prepare processed_text for clustering\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Convert text to numerical format\n",
    "text_features = vectorizer.fit_transform(unlabelled_df['text']).toarray()\n",
    "\n",
    "# Step 4: Fuzzy C-Means Clustering\n",
    "fcm = FCM(n_clusters=2, max_iter=300)  # Assuming 2 clusters for fake and genuine\n",
    "fcm.fit(text_features)\n",
    "\n",
    "# Get cluster labels for unlabelled dataset\n",
    "unlabelled_df['cluster'] = fcm.predict(text_features)\n",
    "\n",
    "# Step 5: Map clusters to labels (fake/genuine)\n",
    "# Assign majority class from the labelled dataset to each cluster\n",
    "cluster_mapping = {}\n",
    "for cluster_id in range(2):\n",
    "    cluster_data = unlabelled_df[unlabelled_df['cluster'] == cluster_id]\n",
    "    cluster_text = cluster_data['text']\n",
    "    matching_labels = labelled_df[labelled_df['text'].isin(cluster_text)]['target']\n",
    "    if not matching_labels.empty:\n",
    "        majority_label = matching_labels.value_counts().idxmax()  # Assign majority label\n",
    "        cluster_mapping[cluster_id] = majority_label\n",
    "    else:\n",
    "        cluster_mapping[cluster_id] = 'unknown'  # Handle clusters with no matches\n",
    "\n",
    "# Map clusters to fake/genuine\n",
    "unlabelled_df['target'] = unlabelled_df['cluster'].map(cluster_mapping)\n",
    "\n",
    "# Step 6: Create processed_balanced_dataset\n",
    "processed_balanced_dataset = unlabelled_df.drop(columns=['cluster'])\n",
    "processed_balanced_dataset.to_csv('processed_balanced_dataset.csv', index=False)\n",
    "\n",
    "# Step 7: Train a model and evaluate performance\n",
    "# Prepare data\n",
    "X = vectorizer.transform(processed_balanced_dataset['text']).toarray()\n",
    "y = LabelEncoder().fit_transform(processed_balanced_dataset['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, target_names=['fake', 'genuine'])\n",
    "\n",
    "# Print results\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Save the classification report to a file\n",
    "with open('classification_report.txt', 'w') as f:\n",
    "    f.write(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4969604863221885\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.50      0.48      0.49     11515\n",
      "     genuine       0.50      0.52      0.51     11515\n",
      "\n",
      "    accuracy                           0.50     23030\n",
      "   macro avg       0.50      0.50      0.50     23030\n",
      "weighted avg       0.50      0.50      0.50     23030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fcmeans import FCM\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Step 2: Download NLTK resources (if you haven't already)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Step 3: Load the dataset without target labels (unlabelled dataset)\n",
    "unlabelled_file = 'dataset_without_target.csv'  # Dataset without target labels\n",
    "unlabelled_df = pd.read_csv(unlabelled_file)\n",
    "\n",
    "# Step 4: Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits (keeping words)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Step 5: Apply text preprocessing to the 'text' column\n",
    "unlabelled_df['processed_text'] = unlabelled_df['processed_text'].apply(preprocess_text)\n",
    "\n",
    "# Step 6: Vectorize the processed text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features\n",
    "text_features = vectorizer.fit_transform(unlabelled_df['processed_text']).toarray()\n",
    "\n",
    "# Step 7: Perform Fuzzy C-Means Clustering\n",
    "n_clusters = 2  # Assuming we are clustering into two groups: fake and genuine\n",
    "fcm = FCM(n_clusters=n_clusters, max_iter=300, random_state=42)  # Initialize FCM\n",
    "fcm.fit(text_features)\n",
    "\n",
    "# Step 8: Assign clusters to the dataset\n",
    "membership_matrix = fcm.u\n",
    "unlabelled_df['cluster'] = membership_matrix.argmax(axis=1)\n",
    "\n",
    "# Step 9: Load the processed (labeled) dataset\n",
    "processed_file = 'processed_balanced_dataset.csv'  # Labeled dataset with actual target\n",
    "processed_df = pd.read_csv(processed_file)\n",
    "\n",
    "# Step 10: Ensure both datasets are aligned by user_id (or review_id)\n",
    "unlabelled_df = unlabelled_df.sort_values(by=\"user_id\").reset_index(drop=True)\n",
    "processed_df = processed_df.sort_values(by=\"user_id\").reset_index(drop=True)\n",
    "\n",
    "# Step 11: Map clusters to labels (cluster 1 -> fake, cluster 0 -> genuine)\n",
    "unlabelled_df['predicted_label'] = unlabelled_df['cluster'].map({0: 'fake', 1: 'genuine'})\n",
    "\n",
    "# Step 12: Compare predicted labels with actual labels\n",
    "y_true = processed_df['target']  # Actual labels from processed dataset\n",
    "y_pred = unlabelled_df['predicted_label']  # Predicted labels from clustering\n",
    "\n",
    "# Step 13: Generate classification report\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred, target_names=['fake', 'genuine'])\n",
    "\n",
    "# Output the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated dataset has been saved to dataset_without_target.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'processed_balanced_dataset.csv'  # Update this to your file's path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the 'target' column\n",
    "df = df.drop(columns=['target'])\n",
    "\n",
    "# Save the updated dataset back to a CSV file\n",
    "output_file_path = 'dataset_without_target.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"The updated dataset has been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94      2292\n",
      "           1       0.97      0.92      0.94      2314\n",
      "\n",
      "    accuracy                           0.94      4606\n",
      "   macro avg       0.94      0.94      0.94      4606\n",
      "weighted avg       0.94      0.94      0.94      4606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class HybridReviewClassifier:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(max_features=500)\n",
    "        self.rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Basic text preprocessing (e.g., lowercasing, removing punctuation, etc.)\n",
    "        return text.lower().strip()\n",
    "\n",
    "    def prepare_features(self, review):\n",
    "        # Extract features like word count, character count, etc.\n",
    "        processed_text = review['processed_text']\n",
    "        features = {\n",
    "            \"word_count\": len(processed_text.split()),\n",
    "            \"char_count\": len(processed_text),\n",
    "            \"avg_word_length\": len(processed_text) / (len(processed_text.split()) + 1e-5),\n",
    "            \"useful_votes\": review.get(\"useful\", 0),\n",
    "            \"funny_votes\": review.get(\"funny\", 0),\n",
    "            \"star_rating\": review.get(\"stars\", 0),\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def fuzzy_score(self, features):\n",
    "        # Placeholder for fuzzy logic score calculation\n",
    "        word_count = features[\"word_count\"]\n",
    "        char_count = features[\"char_count\"]\n",
    "        avg_word_length = features[\"avg_word_length\"]\n",
    "        # Example fuzzy rules\n",
    "        if word_count > 50 and avg_word_length > 4:\n",
    "            return 0.8\n",
    "        elif char_count < 100:\n",
    "            return 0.3\n",
    "        return 0.5\n",
    "\n",
    "    def train(self, train_data):\n",
    "        # Preprocess text and prepare features\n",
    "        # Use 'processed_text' instead of 'text'\n",
    "        tfidf_matrix = self.tfidf.fit_transform(train_data['processed_text'])\n",
    "        \n",
    "        feature_list = []\n",
    "        for _, row in train_data.iterrows():\n",
    "            features = self.prepare_features(row)\n",
    "            features_list = list(features.values())\n",
    "            feature_list.append(features_list)\n",
    "\n",
    "        feature_array = np.array(feature_list)\n",
    "        X_combined = np.hstack([feature_array, tfidf_matrix.toarray()])\n",
    "        y = (train_data['target'] == 'genuine').astype(int)\n",
    "\n",
    "        self.rf_classifier.fit(X_combined, y)\n",
    "\n",
    "    def predict(self, review_data):\n",
    "        # Prepare features for prediction\n",
    "        features = self.prepare_features(review_data)\n",
    "        features_list = np.array(list(features.values())).reshape(1, -1)  # Convert to 2D array\n",
    "        tfidf_features = self.tfidf.transform([review_data['processed_text']]).toarray()\n",
    "        X_combined = np.hstack([features_list, tfidf_features])  # Concatenate 2D arrays\n",
    "\n",
    "        # Hybrid prediction logic\n",
    "        ml_prob = self.rf_classifier.predict_proba(X_combined)[0][1]\n",
    "        fuzzy_score = self.fuzzy_score(features)\n",
    "        final_score = 0.7 * ml_prob + 0.3 * fuzzy_score\n",
    "        return 'genuine' if final_score > 0.5 else 'fake'\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        # Evaluate the model\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        for _, review in test_data.iterrows():\n",
    "            pred = self.predict(review)\n",
    "            actual = review['target'].lower().strip()\n",
    "            predictions.append(1 if pred == 'genuine' else 0)\n",
    "            actuals.append(1 if actual == 'genuine' else 0)\n",
    "        \n",
    "        accuracy = np.mean(np.array(predictions) == np.array(actuals))\n",
    "        return accuracy, predictions\n",
    "\n",
    "def main():\n",
    "    # Load the dataset (modify the path as needed)\n",
    "    dataset = pd.read_csv(\"processed_balanced_dataset.csv\")\n",
    "    \n",
    "    # Split into train and test datasets\n",
    "    train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    classifier = HybridReviewClassifier()\n",
    "    classifier.train(train_data)\n",
    "    accuracy, predictions = classifier.evaluate(test_data)\n",
    "\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "    actuals = (test_data['target'] == 'genuine').astype(int)\n",
    "    print(classification_report(actuals, predictions))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n",
      "Precision: 0.97\n",
      "Recall: 0.91\n",
      "F1 Score: 0.94\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3376   81]\n",
      " [ 315 3137]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94      3457\n",
      "           1       0.97      0.91      0.94      3452\n",
      "\n",
      "    accuracy                           0.94      6909\n",
      "   macro avg       0.94      0.94      0.94      6909\n",
      "weighted avg       0.94      0.94      0.94      6909\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 3137\n",
      "False Positives: 81\n",
      "True Negatives: 3376\n",
      "False Negatives: 315\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from fcmeans import FCM\n",
    "\n",
    "# Step 1: Load dataset\n",
    "dataset = pd.read_csv(\"balanced_dataset.csv\")  # Replace with your file path\n",
    "features = dataset.drop(columns=['target'])\n",
    "target = dataset['target']\n",
    "\n",
    "# Step 2: Normalize numerical columns\n",
    "numeric_columns = ['stars', 'useful', 'funny']  # Adjust as needed\n",
    "scaler = StandardScaler()\n",
    "features_scaled = features.copy()\n",
    "features_scaled[numeric_columns] = scaler.fit_transform(features[numeric_columns])\n",
    "\n",
    "# Step 3: Fuzzy C-Means Clustering\n",
    "# Use only numeric columns for clustering\n",
    "clustering_data = features_scaled[numeric_columns].to_numpy()\n",
    "\n",
    "# Initialize Fuzzy C-Means\n",
    "fcm = FCM(n_clusters=3, m=2)  # Adjust clusters and fuzzifier as needed\n",
    "fcm.fit(clustering_data)\n",
    "\n",
    "# Membership scores for each cluster\n",
    "membership_scores = fcm.u\n",
    "features_scaled[['cluster1', 'cluster2', 'cluster3']] = membership_scores\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X = features_scaled.drop(columns=['user_id', 'review_id', 'text', 'date'])\n",
    "y = (target == 'genuine').astype(int)  # Encode target for binary classification\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 5: Train Classifier\n",
    "clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Predictions and Metrics\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# True Positives, False Positives, True Negatives, False Negatives\n",
    "TP = conf_matrix[1, 1]\n",
    "FP = conf_matrix[0, 1]\n",
    "TN = conf_matrix[0, 0]\n",
    "FN = conf_matrix[1, 0]\n",
    "\n",
    "print(f\"True Positives: {TP}\")\n",
    "print(f\"False Positives: {FP}\")\n",
    "print(f\"True Negatives: {TN}\")\n",
    "print(f\"False Negatives: {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Precision: 0.71\n",
      "Recall: 0.63\n",
      "F1 Score: 0.67\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1703  589]\n",
      " [ 846 1468]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.74      0.70      2292\n",
      "           1       0.71      0.63      0.67      2314\n",
      "\n",
      "    accuracy                           0.69      4606\n",
      "   macro avg       0.69      0.69      0.69      4606\n",
      "weighted avg       0.69      0.69      0.69      4606\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 1468\n",
      "False Positives: 589\n",
      "True Negatives: 1703\n",
      "False Negatives: 846\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from fcmeans import FCM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# Step 1: Load dataset\n",
    "dataset = pd.read_csv(\"processed_balanced_dataset.csv\")  # Replace with your file path\n",
    "features = dataset.drop(columns=['target'])\n",
    "target = dataset['target']\n",
    "\n",
    "# Step 2: Text Vectorization (TF-IDF) with sparse matrix\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Reduce number of features to 1000\n",
    "text_features = vectorizer.fit_transform(features['processed_text'])\n",
    "\n",
    "# Step 3: Dimensionality Reduction (optional)\n",
    "svd = TruncatedSVD(n_components=500)  # Reduce to 500 components\n",
    "text_features_reduced = svd.fit_transform(text_features)\n",
    "\n",
    "# Step 4: Normalize numerical columns\n",
    "numeric_columns = ['useful', 'funny']\n",
    "scaler = StandardScaler()\n",
    "features_scaled = features.copy()\n",
    "features_scaled[numeric_columns] = scaler.fit_transform(features[numeric_columns])\n",
    "\n",
    "# Convert numeric features to sparse format\n",
    "numeric_sparse = csr_matrix(features_scaled[numeric_columns].values)\n",
    "\n",
    "# Step 5: Combine text features and normalized numerical features\n",
    "X = hstack([numeric_sparse, text_features_reduced])\n",
    "\n",
    "# Step 6: Fuzzy C-Means Clustering\n",
    "# Use only numeric columns for clustering\n",
    "clustering_data = features_scaled[numeric_columns].to_numpy()\n",
    "\n",
    "# Initialize Fuzzy C-Means\n",
    "fcm = FCM(n_clusters=2, m=2)  # Adjust clusters and fuzzifier as needed\n",
    "fcm.fit(clustering_data)\n",
    "\n",
    "# Membership scores for each cluster\n",
    "membership_scores = fcm.u\n",
    "features_scaled[['cluster1', 'cluster2']] = membership_scores\n",
    "\n",
    "# Step 7: Train-Test Split\n",
    "y = (target == 'genuine').astype(int)  # Encode target for binary classification\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 8: Train Classifier\n",
    "clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Predictions and Metrics\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# True Positives, False Positives, True Negatives, False Negatives\n",
    "TP = conf_matrix[1, 1]\n",
    "FP = conf_matrix[0, 1]\n",
    "TN = conf_matrix[0, 0]\n",
    "FN = conf_matrix[1, 0]\n",
    "\n",
    "print(f\"True Positives: {TP}\")\n",
    "print(f\"False Positives: {FP}\")\n",
    "print(f\"True Negatives: {TN}\")\n",
    "print(f\"False Negatives: {FN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
