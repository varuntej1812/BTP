{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textstat import flesch_reading_ease\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caluculate Readability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_readability_score(df):\n",
    "    df['readability_fre'] = df['text'].apply(\n",
    "        lambda d: flesch_reading_ease(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Rating Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rating_category(df, threshold):\n",
    "\n",
    "    def assign_rating_category(rating):\n",
    "        if rating > threshold:\n",
    "            return 'positive'\n",
    "        else:\n",
    "            return 'negative'\n",
    "\n",
    "    df['rating_category'] = df['stars'].apply(assign_rating_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pos_tags(df):\n",
    "    def count_pos(Pos_counts, pos_type):\n",
    "        pos_count = Pos_counts.get(pos_type, 0)\n",
    "        return pos_count\n",
    "\n",
    "    def pos_counts(text):\n",
    "        doc = nlp(text)\n",
    "        Pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "        return Pos_counts\n",
    "\n",
    "    poscounts =  df['text'].apply(pos_counts)\n",
    "    df['num_nouns'] = df['text'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.NOUN))\n",
    "    df['num_verbs'] = df['text'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.VERB))\n",
    "    df['num_adjectives'] = df['text'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.ADJ))\n",
    "    df['num_adverbs'] = df['text'].apply(\n",
    "        lambda text: count_pos(poscounts, spacy.parts_of_speech.ADV))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Cosine Similarity with another review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_max_similarity(df):\n",
    "    tfidfvectoriser = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidfvectoriser.fit_transform(df['text'])\n",
    "\n",
    "    cosine_similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    max_similarities = []\n",
    "    for i, row in enumerate(cosine_similarity_matrix):\n",
    "        max_similarity = max(row[:i].tolist() + row[i+1:].tolist())\n",
    "        max_similarities.append(max_similarity)\n",
    "\n",
    "    df['max_similarity'] = max_similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_avg_word_length(df):\n",
    "    def calculate_average_word_length(text):\n",
    "        words = text.split()\n",
    "        total_word_length = sum(len(word) for word in words)\n",
    "        average_word_length = total_word_length / \\\n",
    "            len(words) if len(words) > 0 else 0\n",
    "        return average_word_length \n",
    "    \n",
    "    df['avg_word_length'] = df['text'].apply(calculate_average_word_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df):\n",
    "    add_readability_score(df)\n",
    "    add_rating_category(df, threshold=3.0)\n",
    "    add_pos_tags(df)\n",
    "    add_avg_word_length(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preproess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.lower().split()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def stem_text(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "    words = text.lower().split()\n",
    "    return \" \".join([token.lemma_ for token in nlp(\" \".join([stemmer.stem(word) for word in words if word not in stop_words]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['_id', 'user_id', 'name', 'review_count', 'useful', 'average_stars',\n",
      "       'review_id', 'stars', 'useful_review', 'text', 'date', 'time',\n",
      "       'readability_fre', 'rating_category', 'num_nouns', 'num_verbs',\n",
      "       'num_adjectives', 'num_adverbs', 'avg_word_length'],\n",
      "      dtype='object')\n",
      "                        _id                 user_id    name  review_count  \\\n",
      "0  65ddcc66fa4711915dcb2fbd  qVc8ODYU5SZjKXVBgXdI7w  Walker           585   \n",
      "1  65ddcc66fa4711915dcb2fbe  qVc8ODYU5SZjKXVBgXdI7w  Walker           585   \n",
      "2  65ddcc66fa4711915dcb2fbf  j14WgRoU_-2ZE1aw1dXrJg  Daniel          4333   \n",
      "3  65ddcc66fa4711915dcb2fc0  j14WgRoU_-2ZE1aw1dXrJg  Daniel          4333   \n",
      "4  65ddcc66fa4711915dcb2fc1  j14WgRoU_-2ZE1aw1dXrJg  Daniel          4333   \n",
      "\n",
      "   useful  average_stars               review_id  stars  useful_review  \\\n",
      "0    7217           3.91  Egy2a4qZeXGr2aY6KMxxbg      5              0   \n",
      "1    7217           3.91  01vN0q6aMlFio6HAjLZz7Q      5             30   \n",
      "2   43091           3.74  iMpqDa0Oyukiw5406KYNRw      3              3   \n",
      "3   43091           3.74  3pCHQ8YHkuaZFFAEz7pz6A      4             12   \n",
      "4   43091           3.74  FvTfqugdlzQSvFB1RAafNQ      4              5   \n",
      "\n",
      "                                                text        date      time  \\\n",
      "0  Remarkable food with beach access for the whol...  2017-05-09  23:21:36   \n",
      "1  I loved everything about this lovely train sta...  2009-05-01  02:00:03   \n",
      "2  The Praline Connection makes a mean po' boy.  ...  2010-10-11  19:24:56   \n",
      "3  We walked over to Tennessee Brew Works, one of...  2018-10-23  14:24:32   \n",
      "4  Logan Circle, also known as Logan Square, is a...  2019-08-04  12:29:33   \n",
      "\n",
      "   readability_fre rating_category  \\\n",
      "0            82.04        positive   \n",
      "1            72.53        positive   \n",
      "2            83.96        negative   \n",
      "3            74.49        positive   \n",
      "4            57.87        positive   \n",
      "\n",
      "                                           num_nouns  \\\n",
      "0  {95: 14, 87: 5, 86: 3, 84: 4, 98: 3, 100: 11, ...   \n",
      "1  {95: 14, 87: 5, 86: 3, 84: 4, 98: 3, 100: 11, ...   \n",
      "2  {95: 14, 87: 5, 86: 3, 84: 4, 98: 3, 100: 11, ...   \n",
      "3  {95: 14, 87: 5, 86: 3, 84: 4, 98: 3, 100: 11, ...   \n",
      "4  {95: 14, 87: 5, 86: 3, 84: 4, 98: 3, 100: 11, ...   \n",
      "\n",
      "                                           num_verbs  \\\n",
      "0  {92: 38, 103: 14, 90: 16, 96: 12, 87: 15, 94: ...   \n",
      "1  {92: 38, 103: 14, 90: 16, 96: 12, 87: 15, 94: ...   \n",
      "2  {92: 38, 103: 14, 90: 16, 96: 12, 87: 15, 94: ...   \n",
      "3  {92: 38, 103: 14, 90: 16, 96: 12, 87: 15, 94: ...   \n",
      "4  {92: 38, 103: 14, 90: 16, 96: 12, 87: 15, 94: ...   \n",
      "\n",
      "                                      num_adjectives  \\\n",
      "0  {95: 24, 87: 10, 84: 16, 92: 25, 85: 10, 96: 1...   \n",
      "1  {95: 24, 87: 10, 84: 16, 92: 25, 85: 10, 96: 1...   \n",
      "2  {95: 24, 87: 10, 84: 16, 92: 25, 85: 10, 96: 1...   \n",
      "3  {95: 24, 87: 10, 84: 16, 92: 25, 85: 10, 96: 1...   \n",
      "4  {95: 24, 87: 10, 84: 16, 92: 25, 85: 10, 96: 1...   \n",
      "\n",
      "                                         num_adverbs  avg_word_length  \n",
      "0  {95: 8, 92: 18, 87: 6, 86: 3, 84: 8, 89: 3, 10...         4.342466  \n",
      "1  {95: 8, 92: 18, 87: 6, 86: 3, 84: 8, 89: 3, 10...         5.181818  \n",
      "2  {95: 8, 92: 18, 87: 6, 86: 3, 84: 8, 89: 3, 10...         4.676692  \n",
      "3  {95: 8, 92: 18, 87: 6, 86: 3, 84: 8, 89: 3, 10...         4.931818  \n",
      "4  {95: 8, 92: 18, 87: 6, 86: 3, 84: 8, 89: 3, 10...         5.814917  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('CleanedDataset.csv')\n",
    "# df['text'] = df['text'].apply(preprocess_text(\"Nestled into the end of a strip mall in Montecito, Sakana is a true hidden gem of the locals scene in Santa Barbara. Pulling up to this unassuming and intimate restaurant right next to a Vons, you'll find a dim-lit and charming interior boasting incredibly creative sushi with picture-perfect presentations. \\n\\nLike Nicolette Sheridan, this is what we call a sure thing.\\n\\nFirst, the menu is huge. If you're a strict traditionalist then you'll be disappointed since their spin on sushi is quite novel; combining fish, textures and flavors in ways I had never before seen. We ordered the Surfer's roll...twice...maybe three times because every time it hit the table everyone began using their chopsticks as weapons of war. We went on to devour the Red Dragon and Montecito rolls with impatient precision. The ahi tuna appetizer is also worth noting. The sushi is served in press box form so you won't find any seaweed holding things together or impacting the overall taste. Every dish is positioned like artwork with splashes of colorful sauces striped across and as you eat and dip the image on the plate transforms. \\n\\nIt's not cheap but it's also completely worth the price tag. Plus, they are cool with BYOB so our feast for four ended at under $40 each. For a bonus, see if you can convince your dining companion that the HD fish screen is actually a live feed of the tank in the back kitchen. The fish is fresh enough for it to be convincing...\"))\n",
    "preprocess_features(df)\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'user_id', 'name', 'review_count', 'useful', 'review_id',\n",
       "       'stars', 'useful_review', 'text', 'date', 'time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('btp.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Text Length: 718.2507084481841\n",
      "Threshold Value: 718.2507084481841\n",
      "Number of User IDs with Text Length Above Threshold: 38211\n",
      "Number of User IDs with Text Length Below or Equal to Threshold: 65184\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('btp.csv')\n",
    "\n",
    "# Calculate the length of the text for each user ID\n",
    "data['text_length'] = data['text'].apply(len)\n",
    "\n",
    "average_text_length = data['text_length'].mean()\n",
    "\n",
    "threshold = average_text_length\n",
    "\n",
    "above_threshold_count = (data['text_length'] > threshold).sum()\n",
    "\n",
    "below_threshold_count = (data['text_length'] <= threshold).sum()\n",
    "\n",
    "print(\"Average Text Length:\", average_text_length)\n",
    "print(\"Threshold Value:\", threshold)\n",
    "print(\"Number of User IDs with Text Length Above Threshold:\", above_threshold_count)\n",
    "print(\"Number of User IDs with Text Length Below or Equal to Threshold:\", below_threshold_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
