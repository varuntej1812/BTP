{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, XLNetForSequenceClassification, RobertaTokenizer, XLNetTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "dataset = pd.read_csv('balanced1_dataset.csv')\n",
    "texts = dataset['text'].tolist()\n",
    "labels = dataset['label'].tolist()\n",
    "\n",
    "# Step 2: Load Teacher and Student Models\n",
    "teacher_model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "teacher_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "student_model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased')\n",
    "student_tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "teacher_model.eval()  # Set teacher model to evaluation mode\n",
    "student_model.train()  # Set student model to training mode\n",
    "\n",
    "# Step 3: Tokenize the Dataset\n",
    "teacher_inputs = teacher_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "student_inputs = student_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Step 4: Create DataLoader\n",
    "dataset = TensorDataset(student_inputs['input_ids'], student_inputs['attention_mask'], labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Step 5: Define Distillation Loss Function\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    student_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    loss = F.kl_div(student_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "    return loss\n",
    "\n",
    "# Step 6: Train the Student Model\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(3):  # Example: 3 epochs\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        # Forward pass through the student model\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        # Forward pass through the teacher model\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "        \n",
    "        # Compute distillation loss\n",
    "        loss = distillation_loss(student_logits, teacher_logits)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Step 7: Evaluation Metrics and ROC Curve\n",
    "student_model.eval()  # Set student model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:  # Reuse the same dataloader for evaluation\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Step 8: Calculate Evaluation Metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Step 9: Plot ROC Curve (for binary classification)\n",
    "if len(set(true_labels)) == 2:\n",
    "    roc_auc = roc_auc_score(true_labels, predictions)\n",
    "    print(f\"ROC-AUC Score: {roc_auc}\")\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(true_labels, predictions)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
